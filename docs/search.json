[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vignettes to Embrace Uncertainty",
    "section": "",
    "text": "This is a Quarto book accompanying the Quarto book “Embrace Uncertainty” https://juliamixedmodels.github.io/EmbraceUncertainty/\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  A Collection of Vignettes",
    "section": "",
    "text": "For both kinds the approach is to start with a publication or at least a well documented data set and document the workflow from configuring the data into the required format over necessary transformations to fitting the model. As much as possible visualizations will be provided along the way. Vignettes allow us to reproduced the original analyses but also to go beyond what was reported in the original publication.\nThe expectation is that the collection of vignettes is rich enough to provide starting points for the analyses of new data. Across vignettes there is much variety in initial data wrangling (e.g., renaming of variables according to a style guide, re-ordering factor levels to be compatible with some canned option for a contrast specification, transformations of dependent variables to meet the LMM assumption that model residuals end up being normally distributed).\nAcross vignettes there are many code chunks that illustrate how fixed-effect interactions are visualized. These interactions are based on contrasts specified for factors, but may also involve, for example, polynomial terms of covariates. For interactions involving two covariates there may be a need to convert one of them to a factor for visualization.\nAcross vignettes there is much variety in the visualization of variance components and correlation parameters estimated in the random-effect structure of the LMMs. Usually, we use conditional modes to this end.\nIn summary, the vignettes not only show how an LMM supported by the data is specified and selected among candidate models for the data sets under consideration. This, of course, is critical. Vignettes also provide the context in which this model fitting occurs – both before and after fitting the model."
  },
  {
    "objectID": "contrasts_kwdyz11.html",
    "href": "contrasts_kwdyz11.html",
    "title": "2  Contrast Coding of Visual Attention Effects",
    "section": "",
    "text": "Attach the packages to be used in this vignette."
  },
  {
    "objectID": "contrasts_kwdyz11.html#example-data",
    "href": "contrasts_kwdyz11.html#example-data",
    "title": "2  Contrast Coding of Visual Attention Effects",
    "section": "2.1 Example data",
    "text": "2.1 Example data\nWe take the KWDYZ dataset (Kliegl et al., 2010). This is an experiment looking at three effects of visual cueing under four different cue-target relations (CTRs). Two horizontal rectangles are displayed above and below a central fixation point or they displayed in vertical orientation to the left and right of the fixation point. Subjects react to the onset of a small visual target occuring at one of the four ends of the two rectangles. The target is cued validly on 70% of trials by a brief flash of the corner of the rectangle at which it appears; it is cued invalidly at the three other locations 10% of the trials each.\nWe specify three contrasts for the four-level factor CTR that are derived from spatial, object-based, and attractor-like features of attention. They map onto sequential differences between appropriately ordered factor levels. Interestingly, a different theoretical perspective, derived from feature overlap, leads to a different set of contrasts. Can the results refute one of the theoretical perspectives?\nWe also have a dataset from a replication and extension of this study (Kliegl et al., 2015). Both data sets are also available in R-package RePsychLing"
  },
  {
    "objectID": "contrasts_kwdyz11.html#preprocessing",
    "href": "contrasts_kwdyz11.html#preprocessing",
    "title": "2  Contrast Coding of Visual Attention Effects",
    "section": "2.2 Preprocessing",
    "text": "2.2 Preprocessing\n\ndat1 = @chain \"./data/kwdyz11.arrow\" begin\n  Arrow.Table\n  DataFrame\n  select!(:subj => :Subj, :tar => :CTR, :rt)\nend\ncellmeans = combine(\n  groupby(dat1, [:CTR]),\n  :rt => mean,\n  :rt => std,\n  :rt => length,\n  :rt => (x -> std(x) / sqrt(length(x))) => :rt_semean,\n)\n\n4 rows × 5 columnsCTRrt_meanrt_stdrt_lengthrt_semeanStringFloat64Float64Int64Float641val358.03283.4581201410.5880692sod391.26792.66228631.731773dos405.14692.689328431.738374dod402.395.391428631.78278"
  },
  {
    "objectID": "contrasts_kwdyz11.html#seqdiffcoding",
    "href": "contrasts_kwdyz11.html#seqdiffcoding",
    "title": "2  Contrast Coding of Visual Attention Effects",
    "section": "2.3 SeqDiffCoding",
    "text": "2.3 SeqDiffCoding\n\nform = @formula rt ~ 1 + CTR + (1 + CTR | Subj)\nlevels = [\"val\", \"sod\", \"dos\", \"dod\"]\nm1 = let\n  contrasts = Dict(\n    :CTR => SeqDiffCoding(; levels), :Subj => Grouping()\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\u001b[32mMinimizing 185      Time: 0:00:00 ( 1.43 ms/it)\u001b[39m\n\u001b[34m  objective:  325809.54938125925\u001b[39m\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0900\n54.97\n<1e-99\n55.1891\n\n\nCTR: sod\n33.7818\n3.2873\n10.28\n<1e-24\n23.2478\n\n\nCTR: dos\n13.9851\n2.3060\n6.06\n<1e-08\n10.7577\n\n\nCTR: dod\n-2.7469\n2.2138\n-1.24\n0.2147\n9.5041\n\n\nResidual\n69.8349"
  },
  {
    "objectID": "contrasts_kwdyz11.html#hypothesiscoding",
    "href": "contrasts_kwdyz11.html#hypothesiscoding",
    "title": "2  Contrast Coding of Visual Attention Effects",
    "section": "2.4 HypothesisCoding",
    "text": "2.4 HypothesisCoding\nA general solution (not inverse of last contrast)\n\nm1b = let\n  contrasts = Dict(\n    :CTR => HypothesisCoding(\n      [\n        -1 1 0 0\n        0 -1 1 0\n        0 0 1 -1\n      ];\n      levels,\n      labels=[\"spt\", \"obj\", \"grv\"],\n    ),\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0924\n54.95\n<1e-99\n55.2080\n\n\nCTR: spt\n33.7817\n3.2877\n10.28\n<1e-24\n23.2511\n\n\nCTR: obj\n13.9852\n2.3060\n6.06\n<1e-08\n10.7571\n\n\nCTR: grv\n2.7470\n2.2143\n1.24\n0.2148\n9.5113\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nControlling the ordering of levels for contrasts:\n\nkwarg levels to order the levels; the first is set as the baseline.\nkwarg base= to fix the baseline level.\n\nThe assignment of random factors such as Subj to Grouping() is only necessary when the sample size is very large and leads to an out-of-memory error; it is included only in the first example for reference."
  },
  {
    "objectID": "contrasts_kwdyz11.html#dummycoding",
    "href": "contrasts_kwdyz11.html#dummycoding",
    "title": "2  Contrast Coding of Visual Attention Effects",
    "section": "2.5 DummyCoding",
    "text": "2.5 DummyCoding\n\nm2 = let\n  contrasts = Dict(:CTR => DummyCoding(; base=\"val\"))\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n358.0914\n6.1533\n58.20\n<1e-99\n47.9047\n\n\nCTR: dod\n45.0200\n4.3641\n10.32\n<1e-24\n32.2952\n\n\nCTR: dos\n47.7669\n3.5570\n13.43\n<1e-40\n25.5401\n\n\nCTR: sod\n33.7817\n3.2876\n10.28\n<1e-24\n23.2499\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nThis contrast has the disadvantage that the intercept returns the mean of the level specified as base, default is the first level, not the GM."
  },
  {
    "objectID": "contrasts_kwdyz11.html#specialcoding",
    "href": "contrasts_kwdyz11.html#specialcoding",
    "title": "2  Contrast Coding of Visual Attention Effects",
    "section": "2.6 SpecialCoding",
    "text": "2.6 SpecialCoding\nThe contrasts returned by DummyCoding may be what you want. Can’t we have them, but also the GM rather than the mean of the base level? Yes, we can!\n\nm2b = let\n  contrasts = Dict(\n    :CTR => HypothesisCoding(\n      [\n        -1 1 0 0\n        -1 0 1 0\n        -1 0 0 1\n      ];\n      levels,\n    )\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0894\n54.97\n<1e-99\n55.1850\n\n\nCTR: sod\n33.7817\n3.2873\n10.28\n<1e-24\n23.2475\n\n\nCTR: dos\n47.7669\n3.5574\n13.43\n<1e-40\n25.5440\n\n\nCTR: dod\n45.0200\n4.3638\n10.32\n<1e-24\n32.2928\n\n\nResidual\n69.8349\n\n\n\n\n\n\n\n\n\nJust relevel the factor or move the column with -1s for a different base."
  },
  {
    "objectID": "contrasts_kwdyz11.html#effectscoding",
    "href": "contrasts_kwdyz11.html#effectscoding",
    "title": "2  Contrast Coding of Visual Attention Effects",
    "section": "2.7 EffectsCoding",
    "text": "2.7 EffectsCoding\n\nm3 = let\n  contrasts = Dict(:CTR => EffectsCoding(; base=\"dod\"))\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0910\n54.96\n<1e-99\n55.1970\n\n\nCTR: dos\n16.1248\n1.4404\n11.20\n<1e-28\n7.3310\n\n\nCTR: sod\n2.1396\n1.3337\n1.60\n0.1087\n6.0066\n\n\nCTR: val\n-31.6422\n2.6421\n-11.98\n<1e-32\n19.9492\n\n\nResidual\n69.8349"
  },
  {
    "objectID": "contrasts_kwdyz11.html#helmertcoding",
    "href": "contrasts_kwdyz11.html#helmertcoding",
    "title": "2  Contrast Coding of Visual Attention Effects",
    "section": "2.8 HelmertCoding",
    "text": "2.8 HelmertCoding\n\nm4 = let\n  contrasts = Dict(:CTR => HelmertCoding())\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0927\n54.95\n<1e-99\n55.2103\n\n\nCTR: dos\n1.3735\n1.1077\n1.24\n0.2150\n4.7626\n\n\nCTR: sod\n-4.2039\n0.6847\n-6.14\n<1e-09\n3.3539\n\n\nCTR: val\n-10.5474\n0.8810\n-11.97\n<1e-32\n6.6523\n\n\nResidual\n69.8347"
  },
  {
    "objectID": "contrasts_kwdyz11.html#reverse-helmertcoding",
    "href": "contrasts_kwdyz11.html#reverse-helmertcoding",
    "title": "2  Contrast Coding of Visual Attention Effects",
    "section": "2.9 Reverse HelmertCoding",
    "text": "2.9 Reverse HelmertCoding\n\nm4b = let\n  levels = reverse(StatsModels.levels(dat1.CTR))\n  contrasts = Dict(:CTR => HelmertCoding(; levels))\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0927\n54.95\n<1e-99\n55.2103\n\n\nCTR: dos\n1.3735\n1.1077\n1.24\n0.2150\n4.7626\n\n\nCTR: sod\n-4.2039\n0.6847\n-6.14\n<1e-09\n3.3539\n\n\nCTR: val\n-10.5474\n0.8810\n-11.97\n<1e-32\n6.6523\n\n\nResidual\n69.8347\n\n\n\n\n\n\n\n\n\nHelmert contrasts are othogonal."
  },
  {
    "objectID": "contrasts_kwdyz11.html#anovacoding",
    "href": "contrasts_kwdyz11.html#anovacoding",
    "title": "2  Contrast Coding of Visual Attention Effects",
    "section": "2.10 AnovaCoding",
    "text": "2.10 AnovaCoding\nAnova contrasts are orthogonal.\n\n2.10.1 A(2) x B(2)\nAn A(2) x B(2) design can be recast as an F(4) design with the levels (A1-B1, A1-B2, A2-B1, A2-B2). The following contrast specifiction returns estimates for the main effect of A, the main effect of B, and the interaction of A and B. In a figure With A on the x-axis and the levels of B shown as two lines, the interaction tests the null hypothesis that the two lines are parallel. A positive coefficient implies overadditivity (diverging lines toward the right) and a negative coefficient underadditivity (converging lines).\n\nm5 = let\n  contrasts = Dict(\n    :CTR => HypothesisCoding(\n      [\n        -1 -1 +1 +1          # A\n        -1 +1 -1 +1          # B\n        +1 -1 -1 +1          # A x B\n      ];\n      levels,\n      labels=[\"A\", \"B\", \"AxB\"],\n    ),\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0914\n54.96\n<1e-99\n55.2004\n\n\nCTR: A\n59.0052\n5.1826\n11.39\n<1e-29\n36.2076\n\n\nCTR: B\n31.0348\n4.6748\n6.64\n<1e-10\n31.7114\n\n\nCTR: AxB\n-36.5287\n3.0927\n-11.81\n<1e-31\n16.0050\n\n\nResidual\n69.8348\n\n\n\n\n\n\n\n\n\nIt is also helpful to see the corresponding layout of the four means for the interaction of A and B (i.e., the third contrast)\n        B1     B2\n   A1   +1     -1\n   A2   -1     +1\nThus, interaction tests whether the difference between main diagonal and minor diagonal is different from zero.\n\n\n2.10.2 A(2) x B(2) x C(2)\nGoing beyond the four level factor; it is also helpful to see the corresponding layout of the eight means for the interaction of A and B and C.\n          C1              C2\n      B1     B2        B1     B2\n A1   +1     -1   A1   -1     +1\n A2   -1     +1   A2   +1     -1\n\n\n2.10.3 A(2) x B(2) x C(3)\nTO BE DONE"
  },
  {
    "objectID": "contrasts_kwdyz11.html#nestedcoding",
    "href": "contrasts_kwdyz11.html#nestedcoding",
    "title": "2  Contrast Coding of Visual Attention Effects",
    "section": "2.11 NestedCoding",
    "text": "2.11 NestedCoding\nAn A(2) x B(2) design can be recast as an F(4) design with the levels (A1-B1, A1-B2, A2-B1, A2-B2). The following contrast specifiction returns an estimate for the main effect of A and the effects of B nested in the two levels of A. In a figure With A on the x-axis and the levels of B shown as two lines, the second contrast tests whether A1-B1 is different from A1-B2 and the third contrast tests whether A2-B1 is different from A2-B2.\n\nm8 = let\n  contrasts = Dict(\n    :CTR => HypothesisCoding(\n      [\n        -1 -1 +1 +1\n        -1 +1 0 0\n        0 0 +1 -1\n      ];\n      levels,\n      labels=[\"do_so\", \"spt\", \"grv\"],\n    ),\n  )\n  fit(MixedModel, form, dat1; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n389.7336\n7.0899\n54.97\n<1e-99\n55.1887\n\n\nCTR: do_so\n59.0053\n5.1799\n11.39\n<1e-29\n36.1846\n\n\nCTR: spt\n33.7817\n3.2873\n10.28\n<1e-24\n23.2475\n\n\nCTR: grv\n2.7470\n2.2146\n1.24\n0.2148\n9.5150\n\n\nResidual\n69.8349\n\n\n\n\n\n\n\n\n\nThe three contrasts for one main effect and two nested contrasts are orthogonal. There is no test of the interaction (parallelism)."
  },
  {
    "objectID": "contrasts_kwdyz11.html#other-orthogonal-contrasts",
    "href": "contrasts_kwdyz11.html#other-orthogonal-contrasts",
    "title": "2  Contrast Coding of Visual Attention Effects",
    "section": "2.12 Other orthogonal contrasts",
    "text": "2.12 Other orthogonal contrasts\nFor factors with more than four levels there are many options for specifying orthogonal contrasts as long as one proceeds in a top-down strictly hiearchical fashion.\nSuppose you have a factor with seven levels and let’s ignore shifting colummns. In this case, you have six options for the first contrast, that is 6 vs. 1, 5 vs.2 , 4 vs. 3, 3 vs. 4, 2 vs. 5, and 1 vs. 6 levels. Then, you specify orthogonal contrasts for partitions with more than 2 elements and so on. That is, you don’t specify a contrast that crosses an earlier partition line.\nIn the following example, after an initial 4 vs 3 partitioning of levels, we specify AnovaCoding for the left and HelmertCoding for the right partition.\n\ncontrasts = Dict(\n  :CTR => HypothesisCoding(\n    [\n      -1/4 -1/4 -1/4 -1/4 +1/3 +1/3 +1/3\n      -1/2 -1/2 +1/2 +1/2 0 0 0\n      -1/2 +1/2 -1/2 +1/2 0 0 0\n      +1/2 -1/2 -1/2 +1/2 0 0 0\n      0 0 0 0 -1 +1 0\n      0 0 0 0 -1/2 -1/2 1\n    ];\n    levels=[\"A1\", \"A2\", \"A3\", \"A4\", \"A5\", \"A6\", \"A7\"],\n    labels=[\"c567.1234\", \"B\", \"C\", \"BxC\", \"c6.5\", \"c6.56\"],\n  ),\n);\n\nThere are two rules that hold for all orthogonal contrasts:\n\nThe weights within rows sum to zero.\nFor all pairs of rows, the sum of the products of weights in the same columns sums to zero."
  },
  {
    "objectID": "contrasts_kwdyz11.html#summary-dave-kleinschmidt",
    "href": "contrasts_kwdyz11.html#summary-dave-kleinschmidt",
    "title": "2  Contrast Coding of Visual Attention Effects",
    "section": "2.13 Summary (Dave Kleinschmidt)",
    "text": "2.13 Summary (Dave Kleinschmidt)\nStatsModels\nStatsModels.jl provides a few commonly used contrast coding schemes, some less-commonly used schemes, and structs that allow you to manually specify your own, custom schemes.\n\n2.13.1 Standard contrasts\nThe most commonly used contrasts are DummyCoding and EffectsCoding (which are similar to contr.treatment() and contr.sum() in R, respectively).\n\n\n2.13.2 “Exotic” contrasts\nWe also provide HelmertCoding and SeqDiffCoding\n\n\n2.13.3 Manual contrasts\nContrastsCoding()\nThere are two ways to manually specify contrasts. First, you can specify them directly via ContrastsCoding. If you do, it’s good practice to specify the levels corresponding to the rows of the matrix, although they can be omitted in which case they’ll be inferred from the data.\nHypothesisCoding()\nA better way to specify manual contrasts is via HypothesisCoding, where each row of the matrix corresponds to the weights given to the cell means of the levels corresponding to each column (see Schad et al. (2020) for more information).\n\n\nBaayen, H., Vasishth, S., Kliegl, R., & Bates, D. (2017). The cave\nof shadows: Addressing the human factor with generalized additive mixed\nmodels. Journal of Memory and Language, 94, 206–234.\nhttps://doi.org/10.1016/j.jml.2016.11.006\n\n\nBates, D., Kliegl, R., Vasishth, S., & Baayen, H. (2015).\nParsimonius mixed models. arXiv, 1506.04967v1.\n\n\nFühner, T., Granacher, U., Golle, K., & Kliegl, R. (2021). Age and\nsex effects in physical fitness components of 108,295 third graders\nincluding 515 primary schools and 9 cohorts. Scientific\nReports, 11(1). https://doi.org/10.1038/s41598-021-97000-4\n\n\nKliegl, R., Kushela, J., & Laubrock, J. (2015). Object\norientation and target size modulate the speed of visual attention.\nDepartment of Psychology, University of Potsdam.\n\n\nKliegl, R., Wei, P., Dambacher, M., Yan, M., & Zhou, X. (2010).\nExperimental effects and individual differences in linear mixed models:\nEstimating the relationship between spatial, object, and attraction\neffects in visual attention. Frontiers in Psychology. https://doi.org/10.3389/fpsyg.2010.00238\n\n\nSchad, D. J., Vasishth, S., Hohenstein, S., & Kliegl, R. (2020). How\nto capitalize on a priori contrasts in linear (mixed) models: A\ntutorial. Journal of Memory and Language, 110, 104038.\nhttps://doi.org/10.1016/j.jml.2019.104038"
  },
  {
    "objectID": "contrasts_fggk21.html",
    "href": "contrasts_fggk21.html",
    "title": "3  Contrast Coding of Physical Fitness Effects",
    "section": "",
    "text": "Ths vignette uses a subset of data reported in Fühner, Golle, Granacher, & Kliegl (2021). Age and sex effects in physical fitness components of 108,295 third graders including 515 primary schools and 9 cohorts. (Fühner et al., 2021)\nTo circumvent delays associated with model fitting we work with models that are less complex than those in the reference publication. All the data to reproduce the models in the publication are used here, too; the script requires only a few changes to specify the more complex models in the paper.\nAll children were between 6.0 and 6.99 years at legal keydate (30 September) of school enrollement, that is in their ninth year of life in the third grade.\nThe script is structured in three main sections:"
  },
  {
    "objectID": "contrasts_fggk21.html#setup",
    "href": "contrasts_fggk21.html#setup",
    "title": "3  Contrast Coding of Physical Fitness Effects",
    "section": "3.1 Setup",
    "text": "3.1 Setup\n\n3.1.1 Packages and functions\n\n\nCode\nusing AlgebraOfGraphics\nusing AlgebraOfGraphics: linear\nusing Arrow\nusing CairoMakie\nusing Chain\nusing CategoricalArrays\nusing DataFrames\nusing DataFrameMacros\nif contains(first(Sys.cpu_info()).model, \"Intel\")\n  using MKL             # faster LAPACK on Intel processors\nend\nusing MixedModels\nusing ProgressMeter\nusing Statistics\nusing StatsBase\n\nProgressMeter.ijulia_behavior(:clear);\n\n\n\n\n3.1.2 Readme for ‘./data/fggk21.rds’\nNumber of scores: 525126\n\nCohort: 9 levels; 2011-2019\nSchool: 515 levels\nChild: 108295 levels; all children are between 8.0 and 8.99 years old\nSex: “Girls” (n=55,086), “Boys” (n= 53,209)\nage: testdate - middle of month of birthdate\nTest: 5 levels\n\nEndurance (Run): 6 minute endurance run [m]; to nearest 9m in 9x18m field\nCoordination (Star_r): star coordination run [m/s]; 9x9m field, 4 x diagonal = 50.912 m\nSpeed(S20_r): 20-meters sprint [m/s]\nMuscle power low (SLJ): standing long jump [cm]\nMuscle power up (BPT): 1-kg medicine ball push test [m]\n\nscore - see units\n\n\n\n3.1.3 Preprocessing\n\n3.1.3.1 Read data\n\ntbl = Arrow.Table(\"./data/fggk21.arrow\")\n\nArrow.Table with 525126 rows, 7 columns, and schema:\n :Cohort  String\n :School  String\n :Child   String\n :Sex     String\n :age     Float64\n :Test    String\n :score   Float64\n\n\n\ndf = DataFrame(tbl)\ndescribe(df)\n\n7 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1Cohort201120190String2SchoolS100043S8002000String3ChildC002352C1179660String4Sexfemalemale0String5age8.560737.994528.558529.106090Float646TestBPTStar_r0String7score226.1411.141524.651161530.00Float64\n\n\n\n\n3.1.3.2 Extract a stratified subsample\nWe extract a random sample of 500 children from the Sex (2) x Test (5) cells of the design. Cohort and School are random.\n\nbegin\n  dat = @chain df begin\n    @transform(:Sex = :Sex == \"female\" ? \"Girls\" : \"Boys\")\n    @groupby(:Test, :Sex)\n    combine(x -> x[sample(1:nrow(x), 500), :])\n  end\nend\n\n5,000 rows × 7 columnsTestSexCohortSchoolChildagescoreStringStringStringStringStringFloat64Float641S20_rBoys2017S102052C1070118.982894.651162S20_rBoys2014S102532C0449028.457224.444443S20_rBoys2017S102052C1070118.982894.651164S20_rBoys2011S101059C0599758.580425.05S20_rBoys2018S110504C0347438.358664.081636S20_rBoys2017S106495C0205548.227244.081637S20_rBoys2019S112525C0292498.30395.128218S20_rBoys2016S104978C0239148.257364.444449S20_rBoys2016S104632C0985428.914444.2553210S20_rBoys2019S106227C0116468.136894.2553211S20_rBoys2015S103068C0826068.772074.8780512S20_rBoys2011S102568C0991198.917184.761913S20_rBoys2016S101023C0633928.610544.6511614S20_rBoys2012S100171C0135758.167014.015S20_rBoys2014S103287C0737798.703634.6511616S20_rBoys2014S112938C0937008.870644.5454517S20_rBoys2019S112057C0770188.717324.5454518S20_rBoys2015S105430C0948648.881594.5454519S20_rBoys2014S105491C0263398.290214.4444420S20_rBoys2013S104929C0497538.498295.5555621S20_rBoys2013S102283C0683348.659824.761922S20_rBoys2019S100973C0951878.884334.761923S20_rBoys2013S103925C0308178.331284.5454524S20_rBoys2019S105557C0766678.717324.8780525S20_rBoys2019S103962C0378228.388774.4444426S20_rBoys2016S104905C0460238.46274.4444427S20_rBoys2018S103354C0625618.60784.2553228S20_rBoys2017S104255C0516348.503764.761929S20_rBoys2019S100924C0469968.470914.2553230S20_rBoys2016S100833C0168068.205344.7619⋮⋮⋮⋮⋮⋮⋮⋮\n\n\n\n\n3.1.3.3 Transformations\n\nbegin\n  transform!(dat, :age, :age => (x -> x .- 8.5) => :a1) # centered age (linear)\n  select!(groupby(dat, :Test), :, :score => zscore => :zScore) # z-score\nend\n\n5,000 rows × 9 columnsTestSexCohortSchoolChildagescorea1zScoreStringStringStringStringStringFloat64Float64Float64Float641S20_rBoys2017S102052C1070118.982894.651160.4828880.3159092S20_rBoys2014S102532C0449028.457224.44444-0.0427789-0.186533S20_rBoys2017S102052C1070118.982894.651160.4828880.3159094S20_rBoys2011S101059C0599758.580425.00.08042441.163785S20_rBoys2018S110504C0347438.358664.08163-0.141342-1.068366S20_rBoys2017S106495C0205548.227244.08163-0.272758-1.068367S20_rBoys2019S112525C0292498.30395.12821-0.1960991.475398S20_rBoys2016S104978C0239148.257364.44444-0.242642-0.186539S20_rBoys2016S104632C0985428.914444.255320.414442-0.64620910S20_rBoys2019S106227C0116468.136894.25532-0.363107-0.64620911S20_rBoys2015S103068C0826068.772074.878050.2720740.86736812S20_rBoys2011S102568C0991198.917184.76190.417180.58507413S20_rBoys2016S101023C0633928.610544.651160.1105410.31590914S20_rBoys2012S100171C0135758.167014.0-0.332991-1.2667815S20_rBoys2014S103287C0737798.703634.651160.2036280.31590916S20_rBoys2014S112938C0937008.870644.545450.3706370.058980117S20_rBoys2019S112057C0770188.717324.545450.2173170.058980118S20_rBoys2015S105430C0948648.881594.545450.3815880.058980119S20_rBoys2014S105491C0263398.290214.44444-0.209788-0.1865320S20_rBoys2013S104929C0497538.498295.55556-0.001711162.5140821S20_rBoys2013S102283C0683348.659824.76190.1598220.58507422S20_rBoys2019S100973C0951878.884334.76190.3843260.58507423S20_rBoys2013S103925C0308178.331284.54545-0.168720.058980124S20_rBoys2019S105557C0766678.717324.878050.2173170.86736825S20_rBoys2019S103962C0378228.388774.44444-0.111225-0.1865326S20_rBoys2016S104905C0460238.46274.44444-0.0373032-0.1865327S20_rBoys2018S103354C0625618.60784.255320.107803-0.64620928S20_rBoys2017S104255C0516348.503764.76190.003764540.58507429S20_rBoys2019S100924C0469968.470914.25532-0.0290897-0.64620930S20_rBoys2016S100833C0168068.205344.7619-0.2946610.585074⋮⋮⋮⋮⋮⋮⋮⋮⋮⋮\n\n\n\nbegin\n  dat2 = combine(\n    groupby(dat, [:Test, :Sex]),\n    :score => mean,\n    :score => std,\n    :zScore => mean,\n    :zScore => std,\n  )\nend\n\n10 rows × 6 columnsTestSexscore_meanscore_stdzScore_meanzScore_stdStringStringFloat64Float64Float64Float641S20_rBoys4.573010.4188870.1259511.018132BPTBoys4.0090.6993120.34310.9785233SLJBoys129.91419.96230.202921.028894Star_rBoys2.109820.3038890.1686141.042685RunBoys1048.65158.9690.2878711.053156S20_rGirls4.469370.397541-0.1259510.9662457BPTGirls3.51860.642635-0.34310.8992178SLJGirls122.0418.0015-0.202920.9278319Star_rGirls2.011530.26994-0.1686140.92619910RunGirls961.748128.685-0.2878710.852525\n\n\n\n\n3.1.3.4 Figure of age x Sex x Test interactions\nThe main results of relevance here are shown in Figure 2 of Scientific Reports 11:17566."
  },
  {
    "objectID": "contrasts_fggk21.html#contrast-coding",
    "href": "contrasts_fggk21.html#contrast-coding",
    "title": "3  Contrast Coding of Physical Fitness Effects",
    "section": "3.2 Contrast coding",
    "text": "3.2 Contrast coding\nContrast coding is part of StatsModels.jl. Here is the primary author’s (i.e., Dave Kleinschmidt’s documentation of Modeling Categorical Data.\nThe random factors Child, School, and Cohort are assigned a Grouping contrast. This contrast is needed when the number of groups (i.e., units, levels) is very large. This is the case for Child (i.e., the 108,925 children in the full and probably also the 11,566 children in the reduced data set). The assignment is not necessary for the typical sample size of experiments. However, we use this coding of random factors irrespective of the number of units associated with them to be transparent about the distinction between random and fixed factors.\nA couple of general remarks about the following examples. First, all contrasts defined in this tutorial return an estimate of the Grand Mean (GM) in the intercept, that is they are so-called sum-to-zero contrasts. In both Julia and R the default contrast is Dummy coding which is not a sum-to-zero contrast, but returns the mean of the reference (control) group - unfortunately for (quasi-)experimentally minded scientists.\nSecond, The factor Sex has only two levels. We use EffectCoding (also known as Sum coding in R) to estimate the difference of the levels from the Grand Mean. Unlike in R, the default sign of the effect is for the second level (base is the first, not the last level), but this can be changed with the base kwarg in the command. Effect coding is a sum-to-zero contrast, but when applied to factors with more than two levels does not yield orthogonal contrasts.\nFinally, contrasts for the five levels of the fixed factor Test represent the hypotheses about differences between them. In this tutorial, we use this factor to illustrate various options.\nWe (initially) include only Test as fixed factor and Child as random factor. More complex LMMs can be specified by simply adding other fixed or random factors to the formula.\n\n3.2.1 SeqDiffCoding: contr1\nSeqDiffCoding was used in the publication. This specification tests pairwise differences between the five neighboring levels of Test, that is:\n\nSDC1: 2-1\nSDC2: 3-2\nSDC3: 4-3\nSDC4: 5-4\n\nThe levels were sorted such that these contrasts map onto four a priori hypotheses; in other words, they are theoretically motivated pairwise comparisons. The motivation also encompasses theoretically motivated interactions with Sex. The order of levels can also be explicitly specified during contrast construction. This is very useful if levels are in a different order in the dataframe. We recommend the explicit specification to increase transparency of the code.\nThe statistical disadvantage of SeqDiffCoding is that the contrasts are not orthogonal, that is the contrasts are correlated. This is obvious from the fact that levels 2, 3, and 4 are all used in two contrasts. One consequence of this is that correlation parameters estimated between neighboring contrasts (e.g., 2-1 and 3-2) are difficult to interpret. Usually, they will be negative because assuming some practical limitation on the overall range (e.g., between levels 1 and 3), a small “2-1” effect “correlates” negatively with a larger “3-2” effect for mathematical reasons.\nObviously, the tradeoff between theoretical motivation and statistical purity is something that must be considered carefully when planning the analysis.\n\ncontr1 = merge(\n  Dict(nm => Grouping() for nm in (:School, :Child, :Cohort)),\n  Dict(\n    :Sex => EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n    :Test => SeqDiffCoding(;\n      levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"]\n    ),\n  ),\n)\n\nDict{Symbol, StatsModels.AbstractContrasts} with 5 entries:\n  :Child  => Grouping()\n  :School => Grouping()\n  :Test   => SeqDiffCoding([\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"])\n  :Cohort => Grouping()\n  :Sex    => EffectsCoding(nothing, [\"Girls\", \"Boys\"])\n\n\n\nf_ovi_1 = @formula zScore ~ 1 + Test + (1 | Child);\n\n\nm_ovi_SeqDiff_1 = fit(MixedModel, f_ovi_1, dat; contrasts=contr1)\n\n\u001b[32mMinimizing 12   Time: 0:00:00 (13.58 ms/it)\u001b[39m\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0014\n0.0142\n-0.10\n0.9225\n0.6853\n\n\nTest: Star_r\n-0.0042\n0.0444\n-0.09\n0.9251\n\n\n\nTest: S20_r\n-0.0022\n0.0444\n-0.05\n0.9601\n\n\n\nTest: SLJ\n0.0093\n0.0443\n0.21\n0.8341\n\n\n\nTest: BPT\n-0.0076\n0.0444\n-0.17\n0.8636\n\n\n\nResidual\n0.7262\n\n\n\n\n\n\n\n\n\nIn this case, any differences between tests identified by the contrasts would be spurious because each test was standardized (i.e., M=0, \\(SD\\)=1). The differences could also be due to an imbalance in the number of boys and girls or in the number of missing observations for each test.\nThe primary interest in this study related to interactions of the test contrasts with and age and Sex. We start with age (linear) and its interaction with the four test contrasts.\n\nm_ovi_SeqDiff_2 = let\n  form = @formula zScore ~ 1 + Test * a1 + (1 | Child)\n  fit(MixedModel, form, dat; contrasts=contr1)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0219\n0.0145\n-1.51\n0.1321\n0.6771\n\n\nTest: Star_r\n-0.0415\n0.0453\n-0.92\n0.3601\n\n\n\nTest: S20_r\n0.0095\n0.0452\n0.21\n0.8339\n\n\n\nTest: SLJ\n0.0235\n0.0453\n0.52\n0.6030\n\n\n\nTest: BPT\n-0.0344\n0.0454\n-0.76\n0.4485\n\n\n\na1\n0.3086\n0.0491\n6.29\n<1e-09\n\n\n\nTest: Star_r & a1\n0.5629\n0.1538\n3.66\n0.0003\n\n\n\nTest: S20_r & a1\n-0.1652\n0.1492\n-1.11\n0.2683\n\n\n\nTest: SLJ & a1\n-0.2522\n0.1509\n-1.67\n0.0947\n\n\n\nTest: BPT & a1\n0.4214\n0.1551\n2.72\n0.0066\n\n\n\nResidual\n0.7253\n\n\n\n\n\n\n\n\n\nThe difference between older and younger childrend is larger for Star_r than for Run (0.2473). S20_r did not differ significantly from Star_r (-0.0377) and SLJ (-0.0113) The largest difference in developmental gain was between BPT and SLJ (0.3355).\nPlease note that standard errors of this LMM are anti-conservative because the LMM is missing a lot of information in the RES (e..g., contrast-related VCs snd CPs for Child, School, and Cohort.\nNext we add the main effect of Sex and its interaction with the four test contrasts.\n\nm_ovi_SeqDiff_3 = let\n  form = @formula zScore ~ 1 + Test * (a1 + Sex) + (1 | Child)\n  fit(MixedModel, form, dat; contrasts=contr1)\nend\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0211\n0.0141\n-1.50\n0.1342\n0.6595\n\n\nTest: Star_r\n-0.0403\n0.0440\n-0.92\n0.3595\n\n\n\nTest: S20_r\n0.0055\n0.0439\n0.12\n0.9009\n\n\n\nTest: SLJ\n0.0226\n0.0439\n0.51\n0.6072\n\n\n\nTest: BPT\n-0.0322\n0.0441\n-0.73\n0.4645\n\n\n\na1\n0.2990\n0.0477\n6.27\n<1e-09\n\n\n\nSex: Boys\n0.2242\n0.0137\n16.33\n<1e-59\n\n\n\nTest: Star_r & a1\n0.5485\n0.1496\n3.67\n0.0002\n\n\n\nTest: S20_r & a1\n-0.1149\n0.1452\n-0.79\n0.4286\n\n\n\nTest: SLJ & a1\n-0.2476\n0.1466\n-1.69\n0.0911\n\n\n\nTest: BPT & a1\n0.4010\n0.1506\n2.66\n0.0078\n\n\n\nTest: Star_r & Sex: Boys\n-0.1413\n0.0430\n-3.29\n0.0010\n\n\n\nTest: S20_r & Sex: Boys\n-0.0225\n0.0430\n-0.52\n0.6008\n\n\n\nTest: SLJ & Sex: Boys\n0.0717\n0.0428\n1.68\n0.0939\n\n\n\nTest: BPT & Sex: Boys\n0.1443\n0.0428\n3.37\n0.0007\n\n\n\nResidual\n0.7024\n\n\n\n\n\n\n\n\n\nThe significant interactions with Sex reflect mostly differences related to muscle power, where the physiological constitution gives boys an advantage. The sex difference is smaller when coordination and cognition play a role – as in the Star_r test. (Caveat: SEs are estimated with an underspecified RES.)\nThe final step in this first series is to add the interactions between the three covariates. A significant interaction between any of the four Test contrasts and age (linear) x Sex was hypothesized to reflect a prepubertal signal (i.e., hormones start to rise in girls’ ninth year of life). However, this hypothesis is linked to a specific shape of the interaction: Girls would need to gain more than boys in tests of muscular power.\n\nf_ovi = @formula zScore ~ 1 + Test * a1 * Sex + (1 | Child)\nm_ovi_SeqDiff = fit(MixedModel, f_ovi, dat; contrasts=contr1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0210\n0.0141\n-1.49\n0.1367\n0.6562\n\n\nTest: Star_r\n-0.0406\n0.0440\n-0.92\n0.3568\n\n\n\nTest: S20_r\n0.0048\n0.0439\n0.11\n0.9124\n\n\n\nTest: SLJ\n0.0231\n0.0439\n0.53\n0.5990\n\n\n\nTest: BPT\n-0.0322\n0.0441\n-0.73\n0.4655\n\n\n\na1\n0.2994\n0.0477\n6.28\n<1e-09\n\n\n\nSex: Boys\n0.2281\n0.0141\n16.18\n<1e-58\n\n\n\nTest: Star_r & a1\n0.5476\n0.1498\n3.66\n0.0003\n\n\n\nTest: S20_r & a1\n-0.1173\n0.1453\n-0.81\n0.4192\n\n\n\nTest: SLJ & a1\n-0.2453\n0.1466\n-1.67\n0.0942\n\n\n\nTest: BPT & a1\n0.4008\n0.1506\n2.66\n0.0078\n\n\n\nTest: Star_r & Sex: Boys\n-0.1465\n0.0440\n-3.33\n0.0009\n\n\n\nTest: S20_r & Sex: Boys\n-0.0185\n0.0439\n-0.42\n0.6739\n\n\n\nTest: SLJ & Sex: Boys\n0.0650\n0.0439\n1.48\n0.1393\n\n\n\nTest: BPT & Sex: Boys\n0.1573\n0.0441\n3.57\n0.0004\n\n\n\na1 & Sex: Boys\n-0.0606\n0.0477\n-1.27\n0.2037\n\n\n\nTest: Star_r & a1 & Sex: Boys\n0.0805\n0.1498\n0.54\n0.5908\n\n\n\nTest: S20_r & a1 & Sex: Boys\n-0.0618\n0.1453\n-0.43\n0.6705\n\n\n\nTest: SLJ & a1 & Sex: Boys\n0.1032\n0.1466\n0.70\n0.4814\n\n\n\nTest: BPT & a1 & Sex: Boys\n-0.1943\n0.1506\n-1.29\n0.1969\n\n\n\nResidual\n0.7048\n\n\n\n\n\n\n\n\n\nThe results are very clear: Despite an abundance of statistical power there is no evidence for the differences between boys and girls in how much they gain in the ninth year of life in these five tests. The authors argue that, in this case, absence of evidence looks very much like evidence of absence of a hypothesized interaction.\nIn the next two sections we use different contrasts. Does this have a bearing on this result? We still ignore for now that we are looking at anti-conservative test statistics.\n\n\n3.2.2 HelmertCoding: contr2\nThe second set of contrasts uses HelmertCoding. Helmert coding codes each level as the difference from the average of the lower levels. With the default order of Test levels we get the following test statistics which we describe in reverse order of appearance in model output\n\nHeC4: 5 - mean(1,2,3,4)\nHeC3: 4 - mean(1,2,3)\nHeC2: 3 - mean(1,2)\nHeC1: 2 - 1\n\nIn the model output, HeC1 will be reported first and HeC4 last.\nThere is some justification for the HeC4 specification in a post-hoc manner because the fifth test (BPT) turned out to be different from the other four tests in that high performance is most likely not only related to physical fitness, but also to overweight/obesity, that is for a subset of children high scores on this test might be indicative of physical unfitness. A priori the SDC4 contrast 5-4 between BPT (5) and SLJ (4) was motivated because conceptually both are tests of the physical fitness component Muscular Power, BPT for upper limbs and SLJ for lower limbs, respectively.\nOne could argue that there is justification for HeC3 because Run (1), Star_r (2), and S20 (3) involve running but SLJ (4) does not. Sports scientists, however, recoil. For them it does not make much sense to average the different running tests, because they draw on completely different physiological resources; it is a variant of the old apples-and-oranges problem.\nThe justification for HeC3 is thatRun (1) and Star_r (2) draw more strongly on cardiosrespiratory Endurance than S20 (3) due to the longer duration of the runs compared to sprinting for 20 m which is a pure measure of the physical-fitness component Speed. Again, sports scientists are not very happy with this proposal.\nFinally, HeC1 contrasts the fitness components Endurance, indicated best by Run (1), and Coordination, indicated by Star_r (2). Endurance (i.e., running for 6 minutes) is considered to be the best indicator of health-related status among the five tests because it is a rather pure measure of cardiorespiratory fitness. The Star_r test requires execution of a pre-instructed sequence of forward, sideways, and backward runs. This coordination of body movements implies a demand on working memory (i.e., remembering the order of these subruns) and executive control processes, but performats also depends on endurance. HeC1 yields a measure of Coordination “corrected” for the contribution of Endurance.\nThe statistical advantage of HelmertCoding is that the resulting contrasts are orthogonal (uncorrelated). This allows for optimal partitioning of variance and statistical power. It is also more efficient to estimate “orthogonal” than “non-orthogonal” random-effect structures.\n\ncontr2 = Dict(\n  :School => Grouping(),\n  :Child => Grouping(),\n  :Cohort => Grouping(),\n  :Sex => EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test => HelmertCoding(;\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n  ),\n);\n\n\nm_ovi_Helmert = fit(MixedModel, f_ovi, dat; contrasts=contr2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0210\n0.0141\n-1.49\n0.1367\n0.6562\n\n\nTest: Star_r\n-0.0203\n0.0220\n-0.92\n0.3568\n\n\n\nTest: S20_r\n-0.0052\n0.0127\n-0.41\n0.6843\n\n\n\nTest: SLJ\n0.0032\n0.0090\n0.36\n0.7218\n\n\n\nTest: BPT\n-0.0045\n0.0070\n-0.65\n0.5168\n\n\n\na1\n0.2994\n0.0477\n6.28\n<1e-09\n\n\n\nSex: Boys\n0.2281\n0.0141\n16.18\n<1e-58\n\n\n\nTest: Star_r & a1\n0.2738\n0.0749\n3.66\n0.0003\n\n\n\nTest: S20_r & a1\n0.0522\n0.0420\n1.24\n0.2144\n\n\n\nTest: SLJ & a1\n-0.0352\n0.0304\n-1.16\n0.2468\n\n\n\nTest: BPT & a1\n0.0590\n0.0238\n2.48\n0.0132\n\n\n\nTest: Star_r & Sex: Boys\n-0.0732\n0.0220\n-3.33\n0.0009\n\n\n\nTest: S20_r & Sex: Boys\n-0.0306\n0.0127\n-2.41\n0.0158\n\n\n\nTest: SLJ & Sex: Boys\n0.0010\n0.0090\n0.11\n0.9154\n\n\n\nTest: BPT & Sex: Boys\n0.0320\n0.0070\n4.60\n<1e-05\n\n\n\na1 & Sex: Boys\n-0.0606\n0.0477\n-1.27\n0.2037\n\n\n\nTest: Star_r & a1 & Sex: Boys\n0.0403\n0.0749\n0.54\n0.5908\n\n\n\nTest: S20_r & a1 & Sex: Boys\n-0.0072\n0.0420\n-0.17\n0.8643\n\n\n\nTest: SLJ & a1 & Sex: Boys\n0.0222\n0.0304\n0.73\n0.4656\n\n\n\nTest: BPT & a1 & Sex: Boys\n-0.0255\n0.0238\n-1.07\n0.2834\n\n\n\nResidual\n0.7048\n\n\n\n\n\n\n\n\n\nWe forego a detailed discussion of the effects, but note that again none of the interactions between age x Sex with the four test contrasts was significant.\nThe default labeling of Helmert contrasts may lead to confusions with other contrasts. Therefore, we could provide our own labels:\nlabels=[\"c2.1\", \"c3.12\", \"c4.123\", \"c5.1234\"]\nOnce the order of levels is memorized the proposed labelling is very transparent.\n\n\n3.2.3 HypothesisCoding: contr3\nThe third set of contrasts uses HypothesisCoding. Hypothesis coding allows the user to specify their own a priori contrast matrix, subject to the mathematical constraint that the matrix has full rank. For example, sport scientists agree that the first four tests can be contrasted with BPT, because the difference is akin to a correction of overall physical fitness. However, they want to keep the pairwise comparisons for the first four tests.\n\nHyC1: BPT - mean(1,2,3,4)\nHyC2: Star_r - Run_r\nHyC3: Run_r - S20_r\nHyC4: S20_r - SLJ\n\n\ncontr3 = Dict(\n  :School => Grouping(),\n  :Child => Grouping(),\n  :Cohort => Grouping(),\n  :Sex => EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test => HypothesisCoding(\n    [\n      -1 -1 -1 -1 +4\n      -1 +1 0 0 0\n       0 -1 +1 0 0\n       0 0 -1 +1 0\n    ];\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n    labels=[\"BPT-other\", \"Star-End\", \"S20-Star\", \"SLJ-S20\"],\n  ),\n);\n\n\nm_ovi_Hypo = fit(MixedModel, f_ovi, dat; contrasts=contr3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0210\n0.0141\n-1.49\n0.1367\n0.6562\n\n\nTest: BPT-other\n-0.0902\n0.1392\n-0.65\n0.5168\n\n\n\nTest: Star-End\n-0.0406\n0.0440\n-0.92\n0.3568\n\n\n\nTest: S20-Star\n0.0048\n0.0439\n0.11\n0.9124\n\n\n\nTest: SLJ-S20\n0.0231\n0.0439\n0.53\n0.5990\n\n\n\na1\n0.2994\n0.0477\n6.28\n<1e-09\n\n\n\nSex: Boys\n0.2281\n0.0141\n16.18\n<1e-58\n\n\n\nTest: BPT-other & a1\n1.1804\n0.4762\n2.48\n0.0132\n\n\n\nTest: Star-End & a1\n0.5476\n0.1498\n3.66\n0.0003\n\n\n\nTest: S20-Star & a1\n-0.1173\n0.1453\n-0.81\n0.4192\n\n\n\nTest: SLJ-S20 & a1\n-0.2453\n0.1466\n-1.67\n0.0942\n\n\n\nTest: BPT-other & Sex: Boys\n0.6406\n0.1392\n4.60\n<1e-05\n\n\n\nTest: Star-End & Sex: Boys\n-0.1465\n0.0440\n-3.33\n0.0009\n\n\n\nTest: S20-Star & Sex: Boys\n-0.0185\n0.0439\n-0.42\n0.6739\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0650\n0.0439\n1.48\n0.1393\n\n\n\na1 & Sex: Boys\n-0.0606\n0.0477\n-1.27\n0.2037\n\n\n\nTest: BPT-other & a1 & Sex: Boys\n-0.5109\n0.4762\n-1.07\n0.2834\n\n\n\nTest: Star-End & a1 & Sex: Boys\n0.0805\n0.1498\n0.54\n0.5908\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n-0.0618\n0.1453\n-0.43\n0.6705\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n0.1032\n0.1466\n0.70\n0.4814\n\n\n\nResidual\n0.7048\n\n\n\n\n\n\n\n\n\nWith HypothesisCoding we must generate our own labels for the contrasts. The default labeling of contrasts is usually not interpretable. Therefore, we provide our own.\nAnyway, none of the interactions between age x Sex with the four Test contrasts was significant for these contrasts.\n\ncontr1b = Dict(\n  :School => Grouping(),\n  :Child => Grouping(),\n  :Cohort => Grouping(),\n  :Sex => EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test => HypothesisCoding(\n    [\n      -1 +1 0 0 0\n      0 -1 +1 0 0\n      0 0 -1 +1 0\n      0 0 0 -1 +1\n    ];\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n    labels=[\"Star-Run\", \"S20-Star\", \"SLJ-S20\", \"BPT-SLJ\"],\n  ),\n);\n\n\nm_ovi_SeqDiff_v2 = fit(MixedModel, f_ovi, dat; contrasts=contr1b)\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0210\n0.0141\n-1.49\n0.1367\n0.6562\n\n\nTest: Star-Run\n-0.0406\n0.0440\n-0.92\n0.3568\n\n\n\nTest: S20-Star\n0.0048\n0.0439\n0.11\n0.9124\n\n\n\nTest: SLJ-S20\n0.0231\n0.0439\n0.53\n0.5990\n\n\n\nTest: BPT-SLJ\n-0.0322\n0.0441\n-0.73\n0.4655\n\n\n\na1\n0.2994\n0.0477\n6.28\n<1e-09\n\n\n\nSex: Boys\n0.2281\n0.0141\n16.18\n<1e-58\n\n\n\nTest: Star-Run & a1\n0.5476\n0.1498\n3.66\n0.0003\n\n\n\nTest: S20-Star & a1\n-0.1173\n0.1453\n-0.81\n0.4192\n\n\n\nTest: SLJ-S20 & a1\n-0.2453\n0.1466\n-1.67\n0.0942\n\n\n\nTest: BPT-SLJ & a1\n0.4008\n0.1506\n2.66\n0.0078\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1465\n0.0440\n-3.33\n0.0009\n\n\n\nTest: S20-Star & Sex: Boys\n-0.0185\n0.0439\n-0.42\n0.6739\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0650\n0.0439\n1.48\n0.1393\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.1573\n0.0441\n3.57\n0.0004\n\n\n\na1 & Sex: Boys\n-0.0606\n0.0477\n-1.27\n0.2037\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n0.0805\n0.1498\n0.54\n0.5908\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n-0.0618\n0.1453\n-0.43\n0.6705\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n0.1032\n0.1466\n0.70\n0.4814\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n-0.1943\n0.1506\n-1.29\n0.1969\n\n\n\nResidual\n0.7048\n\n\n\n\n\n\n\n\n\n\nm_zcp_SeqD = let\n  form = @formula(\n    zScore ~ 1 + Test * a1 * Sex + zerocorr(1 + Test | Child)\n  )\n  fit(MixedModel, form, dat; contrasts=contr1b)\nend\n\n\u001b[32mMinimizing 87   Time: 0:00:00 ( 4.24 ms/it)\u001b[39m\n\u001b[34m  objective:  13783.535128465079\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0209\n0.0141\n-1.48\n0.1387\n0.6769\n\n\nTest: Star-Run\n-0.0444\n0.0444\n-1.00\n0.3178\n0.0000\n\n\nTest: S20-Star\n0.0064\n0.0438\n0.15\n0.8846\n0.4864\n\n\nTest: SLJ-S20\n0.0252\n0.0435\n0.58\n0.5621\n0.4025\n\n\nTest: BPT-SLJ\n-0.0340\n0.0440\n-0.77\n0.4400\n0.0000\n\n\na1\n0.2995\n0.0477\n6.28\n<1e-09\n\n\n\nSex: Boys\n0.2282\n0.0141\n16.18\n<1e-58\n\n\n\nTest: Star-Run & a1\n0.5549\n0.1511\n3.67\n0.0002\n\n\n\nTest: S20-Star & a1\n-0.1221\n0.1448\n-0.84\n0.3989\n\n\n\nTest: SLJ-S20 & a1\n-0.2481\n0.1453\n-1.71\n0.0876\n\n\n\nTest: BPT-SLJ & a1\n0.4077\n0.1504\n2.71\n0.0067\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1454\n0.0444\n-3.27\n0.0011\n\n\n\nTest: S20-Star & Sex: Boys\n-0.0197\n0.0438\n-0.45\n0.6518\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0644\n0.0435\n1.48\n0.1393\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.1557\n0.0440\n3.54\n0.0004\n\n\n\na1 & Sex: Boys\n-0.0599\n0.0477\n-1.26\n0.2088\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n0.0776\n0.1511\n0.51\n0.6078\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n-0.0602\n0.1448\n-0.42\n0.6777\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n0.1034\n0.1453\n0.71\n0.4765\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n-0.1973\n0.1504\n-1.31\n0.1896\n\n\n\nResidual\n0.6116\n\n\n\n\n\n\n\n\n\n\nm_zcp_SeqD_2 = let\n  form = @formula(\n    zScore ~ 1 + Test * a1 * Sex + (0 + Test | Child)\n  )\n  fit(MixedModel, form, dat; contrasts=contr1b)\nend\n\n\u001b[32mMinimizing 2755     Time: 0:00:11 ( 4.01 ms/it)\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0198\n0.0140\n-1.41\n0.1579\n\n\n\nTest: Star-Run\n-0.0360\n0.0439\n-0.82\n0.4115\n\n\n\nTest: S20-Star\n0.0057\n0.0448\n0.13\n0.8996\n\n\n\nTest: SLJ-S20\n0.0272\n0.0448\n0.61\n0.5436\n\n\n\nTest: BPT-SLJ\n-0.0343\n0.0436\n-0.79\n0.4314\n\n\n\na1\n0.3022\n0.0474\n6.37\n<1e-09\n\n\n\nSex: Boys\n0.2293\n0.0140\n16.32\n<1e-59\n\n\n\nTest: Star-Run & a1\n0.5547\n0.1492\n3.72\n0.0002\n\n\n\nTest: S20-Star & a1\n-0.1189\n0.1485\n-0.80\n0.4236\n\n\n\nTest: SLJ-S20 & a1\n-0.2649\n0.1494\n-1.77\n0.0763\n\n\n\nTest: BPT-SLJ & a1\n0.4130\n0.1491\n2.77\n0.0056\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1445\n0.0439\n-3.29\n0.0010\n\n\n\nTest: S20-Star & Sex: Boys\n-0.0164\n0.0448\n-0.37\n0.7139\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0533\n0.0448\n1.19\n0.2336\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.1590\n0.0436\n3.65\n0.0003\n\n\n\na1 & Sex: Boys\n-0.0587\n0.0474\n-1.24\n0.2156\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n0.0959\n0.1492\n0.64\n0.5203\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n-0.0995\n0.1485\n-0.67\n0.5028\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n0.1205\n0.1494\n0.81\n0.4200\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n-0.2088\n0.1491\n-1.40\n0.1615\n\n\n\nTest: BPT\n\n\n\n\n0.9260\n\n\nTest: SLJ\n\n\n\n\n0.9790\n\n\nTest: Star_r\n\n\n\n\n0.9714\n\n\nTest: Run\n\n\n\n\n0.9476\n\n\nTest: S20_r\n\n\n\n\n0.9832\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nm_cpx_0_SeqDiff = let\n  f_cpx_0 = @formula(\n    zScore ~ 1 + Test * a1 * Sex + (0 + Test | Child)\n  )\n  fit(MixedModel, f_cpx_0, dat; contrasts=contr1b)\nend\n\n\u001b[32mMinimizing 2755     Time: 0:00:11 ( 4.02 ms/it)\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0198\n0.0140\n-1.41\n0.1579\n\n\n\nTest: Star-Run\n-0.0360\n0.0439\n-0.82\n0.4115\n\n\n\nTest: S20-Star\n0.0057\n0.0448\n0.13\n0.8996\n\n\n\nTest: SLJ-S20\n0.0272\n0.0448\n0.61\n0.5436\n\n\n\nTest: BPT-SLJ\n-0.0343\n0.0436\n-0.79\n0.4314\n\n\n\na1\n0.3022\n0.0474\n6.37\n<1e-09\n\n\n\nSex: Boys\n0.2293\n0.0140\n16.32\n<1e-59\n\n\n\nTest: Star-Run & a1\n0.5547\n0.1492\n3.72\n0.0002\n\n\n\nTest: S20-Star & a1\n-0.1189\n0.1485\n-0.80\n0.4236\n\n\n\nTest: SLJ-S20 & a1\n-0.2649\n0.1494\n-1.77\n0.0763\n\n\n\nTest: BPT-SLJ & a1\n0.4130\n0.1491\n2.77\n0.0056\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1445\n0.0439\n-3.29\n0.0010\n\n\n\nTest: S20-Star & Sex: Boys\n-0.0164\n0.0448\n-0.37\n0.7139\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0533\n0.0448\n1.19\n0.2336\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.1590\n0.0436\n3.65\n0.0003\n\n\n\na1 & Sex: Boys\n-0.0587\n0.0474\n-1.24\n0.2156\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n0.0959\n0.1492\n0.64\n0.5203\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n-0.0995\n0.1485\n-0.67\n0.5028\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n0.1205\n0.1494\n0.81\n0.4200\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n-0.2088\n0.1491\n-1.40\n0.1615\n\n\n\nTest: BPT\n\n\n\n\n0.9260\n\n\nTest: SLJ\n\n\n\n\n0.9790\n\n\nTest: Star_r\n\n\n\n\n0.9714\n\n\nTest: Run\n\n\n\n\n0.9476\n\n\nTest: S20_r\n\n\n\n\n0.9832\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpx_0_SeqDiff)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\nTest: Run\n0.89792563\n0.94758938\n\n\n\n\n\n\n\nTest: Star_r\n0.94365633\n0.97141975\n+0.45\n\n\n\n\n\n\nTest: S20_r\n0.96674010\n0.98322942\n+0.51\n-0.54\n\n\n\n\n\nTest: SLJ\n0.95841645\n0.97898746\n+0.69\n+0.21\n+0.45\n\n\n\n\nTest: BPT\n0.85748699\n0.92600593\n+0.42\n-0.06\n+0.46\n+0.51\n\n\nResidual\n\n0.00000000\n0.00004047\n\n\n\n\n\n\n\n\n\n\nm_cpx_0_SeqDiff.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n Test: Run      1.0     .      .      .      .\n Test: Star_r   0.45   1.0     .      .      .\n Test: S20_r    0.51  -0.54   1.0     .      .\n Test: SLJ      0.69   0.21   0.45   1.0     .\n Test: BPT      0.42  -0.06   0.46   0.51   1.0\n\nNormalized cumulative variances:\n[0.5071, 0.8176, 0.9335, 1.0, 1.0]\n\nComponent loadings\n                 PC1    PC2    PC3    PC4    PC5\n Test: Run     -0.54  -0.29  -0.35   0.43  -0.56\n Test: Star_r  -0.06  -0.79   0.05   0.17   0.58\n Test: S20_r   -0.46   0.49  -0.38   0.23   0.6\n Test: SLJ     -0.54  -0.15  -0.06  -0.83  -0.01\n Test: BPT     -0.46   0.13   0.85   0.22   0.0,)\n\n\n\nf_cpx_1 = @formula(\n  zScore ~ 1 + Test * a1 * Sex + (1 + Test | Child)\n)\nm_cpx_1_SeqDiff =\nfit(MixedModel, f_cpx_1, dat; contrasts=contr1b)\n\n\u001b[32mMinimizing 2791     Time: 0:00:11 ( 4.02 ms/it)\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0206\n0.0141\n-1.47\n0.1428\n0.6882\n\n\nTest: Star-Run\n-0.0402\n0.0440\n-0.91\n0.3603\n0.9584\n\n\nTest: S20-Star\n0.0057\n0.0445\n0.13\n0.8974\n1.1327\n\n\nTest: SLJ-S20\n0.0317\n0.0445\n0.71\n0.4759\n0.8575\n\n\nTest: BPT-SLJ\n-0.0409\n0.0434\n-0.94\n0.3450\n0.7920\n\n\na1\n0.3024\n0.0476\n6.35\n<1e-09\n\n\n\nSex: Boys\n0.2277\n0.0141\n16.17\n<1e-58\n\n\n\nTest: Star-Run & a1\n0.5540\n0.1495\n3.71\n0.0002\n\n\n\nTest: S20-Star & a1\n-0.1225\n0.1472\n-0.83\n0.4053\n\n\n\nTest: SLJ-S20 & a1\n-0.2538\n0.1484\n-1.71\n0.0874\n\n\n\nTest: BPT-SLJ & a1\n0.4251\n0.1485\n2.86\n0.0042\n\n\n\nTest: Star-Run & Sex: Boys\n-0.1463\n0.0440\n-3.33\n0.0009\n\n\n\nTest: S20-Star & Sex: Boys\n-0.0216\n0.0445\n-0.49\n0.6274\n\n\n\nTest: SLJ-S20 & Sex: Boys\n0.0604\n0.0445\n1.36\n0.1751\n\n\n\nTest: BPT-SLJ & Sex: Boys\n0.1560\n0.0434\n3.60\n0.0003\n\n\n\na1 & Sex: Boys\n-0.0580\n0.0476\n-1.22\n0.2231\n\n\n\nTest: Star-Run & a1 & Sex: Boys\n0.0873\n0.1495\n0.58\n0.5593\n\n\n\nTest: S20-Star & a1 & Sex: Boys\n-0.0764\n0.1472\n-0.52\n0.6039\n\n\n\nTest: SLJ-S20 & a1 & Sex: Boys\n0.1031\n0.1484\n0.69\n0.4874\n\n\n\nTest: BPT-SLJ & a1 & Sex: Boys\n-0.2044\n0.1485\n-1.38\n0.1686\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nm_cpx_1_SeqDiff.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)      1.0     .      .      .      .\n Test: Star-Run  -0.3    1.0     .      .      .\n Test: S20-Star   0.08  -0.65   1.0     .      .\n Test: SLJ-S20    0.25  -0.18  -0.3    1.0     .\n Test: BPT-SLJ   -0.48   0.05  -0.58   0.55   1.0\n\nNormalized cumulative variances:\n[0.4363, 0.7336, 0.9459, 0.9984, 1.0]\n\nComponent loadings\n                   PC1    PC2    PC3    PC4    PC5\n (Intercept)     -0.3   -0.42   0.7   -0.27   0.41\n Test: Star-Run   0.38   0.54   0.43   0.47   0.4\n Test: S20-Star  -0.59  -0.04  -0.41   0.46   0.52\n Test: SLJ-S20    0.3   -0.68   0.09   0.63  -0.21\n Test: BPT-SLJ    0.57  -0.27  -0.39  -0.31   0.6,)\n\n\n\n\n3.2.4 PCA-based HypothesisCoding: contr4\nThe fourth set of contrasts uses HypothesisCoding to specify the set of contrasts implementing the loadings of the four principle components of the published LMM based on test scores, not test effects (contrasts) - coarse-grained, that is roughly according to their signs. This is actually a very interesting and plausible solution nobody had proposed a priori.\n\nPC1: BPT - Run_r\nPC2: (Star_r + S20_r + SLJ) - (BPT + Run_r)\nPC3: Star_r - (S20_r + SLJ)\nPC4: S20_r - SLJ\n\nPC1 contrasts the worst and the best indicator of physical health; PC2 contrasts these two against the core indicators of physical fitness; PC3 contrasts the cognitive and the physical tests within the narrow set of physical fitness components; and PC4, finally, contrasts two types of lower muscular fitness differing in speed and power.\n\ncontr4 = Dict(\n  :School => Grouping(),\n  :Child => Grouping(),\n  :Cohort => Grouping(),\n  :Sex => EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n  :Test => HypothesisCoding(\n    [\n      -1 0 0 0 +1\n      -3 +2 +2 +2 -3\n      0 +2 -1 -1 0\n      0 0 +1 -1 0\n    ];\n    levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n    labels=[\"c5.1\", \"c234.15\", \"c2.34\", \"c3.4\"],\n  ),\n);\n\n\nm_cpx_1_PC = fit(MixedModel, f_cpx_1, dat; contrasts=contr4)\n\n\u001b[32mMinimizing 1557     Time: 0:00:06 ( 4.01 ms/it)\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0205\n0.0141\n-1.45\n0.1457\n0.7005\n\n\nTest: c5.1\n-0.0486\n0.0442\n-1.10\n0.2720\n1.1581\n\n\nTest: c234.15\n-0.0446\n0.1701\n-0.26\n0.7930\n0.7632\n\n\nTest: c2.34\n-0.0284\n0.0759\n-0.37\n0.7083\n1.7006\n\n\nTest: c3.4\n-0.0240\n0.0443\n-0.54\n0.5883\n1.4273\n\n\na1\n0.3008\n0.0477\n6.31\n<1e-09\n\n\n\nSex: Boys\n0.2281\n0.0141\n16.18\n<1e-58\n\n\n\nTest: c5.1 & a1\n0.6098\n0.1521\n4.01\n<1e-04\n\n\n\nTest: c234.15 & a1\n0.5428\n0.5788\n0.94\n0.3483\n\n\n\nTest: c2.34 & a1\n0.4829\n0.2531\n1.91\n0.0564\n\n\n\nTest: c3.4 & a1\n0.2752\n0.1481\n1.86\n0.0631\n\n\n\nTest: c5.1 & Sex: Boys\n0.0555\n0.0442\n1.26\n0.2091\n\n\n\nTest: c234.15 & Sex: Boys\n-1.0368\n0.1701\n-6.09\n<1e-08\n\n\n\nTest: c2.34 & Sex: Boys\n-0.0349\n0.0759\n-0.46\n0.6454\n\n\n\nTest: c3.4 & Sex: Boys\n-0.0634\n0.0443\n-1.43\n0.1528\n\n\n\na1 & Sex: Boys\n-0.0583\n0.0477\n-1.22\n0.2218\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n-0.0575\n0.1521\n-0.38\n0.7056\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n0.5697\n0.5788\n0.98\n0.3250\n\n\n\nTest: c2.34 & a1 & Sex: Boys\n-0.0274\n0.2531\n-0.11\n0.9139\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n-0.1126\n0.1481\n-0.76\n0.4471\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpx_1_PC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.49071876\n0.70051321\n\n\n\n\n\n\n\nTest: c5.1\n1.34123345\n1.15811634\n-0.04\n\n\n\n\n\n\nTest: c234.15\n0.58240598\n0.76315528\n-0.96\n-0.24\n\n\n\n\n\nTest: c2.34\n2.89197628\n1.70058116\n+0.24\n+0.31\n-0.32\n\n\n\n\nTest: c3.4\n2.03721883\n1.42731175\n+0.04\n-0.09\n-0.02\n+0.16\n\n\nResidual\n\n0.00000000\n0.00002443\n\n\n\n\n\n\n\n\n\n\nm_cpx_1_PC.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)     1.0     .      .      .      .\n Test: c5.1     -0.04   1.0     .      .      .\n Test: c234.15  -0.96  -0.24   1.0     .      .\n Test: c2.34     0.24   0.31  -0.32   1.0     .\n Test: c3.4      0.04  -0.09  -0.02   0.16   1.0\n\nNormalized cumulative variances:\n[0.4321, 0.6678, 0.882, 1.0, 1.0]\n\nComponent loadings\n                  PC1    PC2    PC3    PC4    PC5\n (Intercept)    -0.62   0.39   0.07   0.02  -0.68\n Test: c5.1     -0.21  -0.76   0.23   0.54  -0.2\n Test: c234.15   0.66  -0.16  -0.13  -0.17  -0.7\n Test: c2.34    -0.37  -0.5   -0.3   -0.72  -0.0\n Test: c3.4     -0.07   0.02  -0.91   0.4   -0.0,)\n\n\nThere is a numerical interaction with a z-value > 2.0 for the first PCA (i.e., BPT - Run_r). This interaction would really need to be replicated to be taken seriously. It is probably due to larger “unfitness” gains in boys than girls (i.e., in BPT) relative to the slightly larger health-related “fitness” gains of girls than boys (i.e., in Run_r).\n\ncontr4b = merge(\n  Dict(nm => Grouping() for nm in (:School, :Child, :Cohort)),\n  Dict(\n    :Sex => EffectsCoding(; levels=[\"Girls\", \"Boys\"]),\n    :Test => HypothesisCoding(\n      [\n        0.49 -0.04 0.20 0.03 -0.85\n        0.70 -0.56 -0.21 -0.13 0.37\n        0.31 0.68 -0.56 -0.35 0.00\n        0.04 0.08 0.61 -0.78 0.13\n      ];\n      levels=[\"Run\", \"Star_r\", \"S20_r\", \"SLJ\", \"BPT\"],\n      labels=[\"c5.1\", \"c234.15\", \"c12.34\", \"c3.4\"],\n    ),\n  ),\n);\n\n\nm_cpx_1_PC_2 = fit(MixedModel, f_cpx_1, dat; contrasts=contr4b)\n\n\u001b[32mMinimizing 1930     Time: 0:00:07 ( 4.04 ms/it)\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0201\n0.0142\n-1.41\n0.1585\n0.6752\n\n\nTest: c5.1\n0.0256\n0.0304\n0.84\n0.3986\n0.7371\n\n\nTest: c234.15\n0.0176\n0.0311\n0.56\n0.5723\n0.5569\n\n\nTest: c12.34\n-0.0046\n0.0316\n-0.15\n0.8837\n0.9707\n\n\nTest: c3.4\n-0.0207\n0.0317\n-0.65\n0.5137\n0.7406\n\n\na1\n0.2928\n0.0480\n6.10\n<1e-08\n\n\n\nSex: Boys\n0.2191\n0.0142\n15.39\n<1e-52\n\n\n\nTest: c5.1 & a1\n-0.3740\n0.1042\n-3.59\n0.0003\n\n\n\nTest: c234.15 & a1\n-0.2593\n0.1059\n-2.45\n0.0143\n\n\n\nTest: c12.34 & a1\n0.0470\n0.1054\n0.45\n0.6556\n\n\n\nTest: c3.4 & a1\n0.2219\n0.1065\n2.08\n0.0372\n\n\n\nTest: c5.1 & Sex: Boys\n-0.0890\n0.0304\n-2.93\n0.0034\n\n\n\nTest: c234.15 & Sex: Boys\n0.1631\n0.0311\n5.24\n<1e-06\n\n\n\nTest: c12.34 & Sex: Boys\n0.0333\n0.0316\n1.05\n0.2915\n\n\n\nTest: c3.4 & Sex: Boys\n-0.0147\n0.0317\n-0.46\n0.6438\n\n\n\na1 & Sex: Boys\n-0.0514\n0.0480\n-1.07\n0.2841\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n0.0784\n0.1042\n0.75\n0.4518\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n-0.1011\n0.1059\n-0.95\n0.3396\n\n\n\nTest: c12.34 & a1 & Sex: Boys\n0.0155\n0.1054\n0.15\n0.8829\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n-0.1009\n0.1065\n-0.95\n0.3434\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_cpx_1_PC_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.45595965\n0.67524784\n\n\n\n\n\n\n\nTest: c5.1\n0.54333330\n0.73711146\n+0.12\n\n\n\n\n\n\nTest: c234.15\n0.31016528\n0.55692484\n+0.27\n-0.02\n\n\n\n\n\nTest: c12.34\n0.94233140\n0.97073755\n-0.02\n-0.15\n-0.26\n\n\n\n\nTest: c3.4\n0.54847084\n0.74058817\n-0.19\n+0.07\n-0.13\n-0.19\n\n\nResidual\n\n0.00000000\n0.00003921\n\n\n\n\n\n\n\n\n\n\nm_cpx_1_PC_2.PCA\n\n(Child = \nPrincipal components based on correlation matrix\n (Intercept)     1.0     .      .      .      .\n Test: c5.1      0.12   1.0     .      .      .\n Test: c234.15   0.27  -0.02   1.0     .      .\n Test: c12.34   -0.02  -0.15  -0.26   1.0     .\n Test: c3.4     -0.19   0.07  -0.13  -0.19   1.0\n\nNormalized cumulative variances:\n[0.2874, 0.5448, 0.7438, 0.8876, 1.0]\n\nComponent loadings\n                  PC1    PC2    PC3    PC4    PC5\n (Intercept)    -0.58   0.25  -0.29   0.62   0.35\n Test: c5.1     -0.21  -0.37  -0.82  -0.27  -0.29\n Test: c234.15  -0.65   0.01   0.4   -0.05  -0.64\n Test: c12.34    0.38   0.6   -0.27   0.3   -0.57\n Test: c3.4      0.24  -0.66   0.11   0.67  -0.23,)"
  },
  {
    "objectID": "contrasts_fggk21.html#other-topics",
    "href": "contrasts_fggk21.html#other-topics",
    "title": "3  Contrast Coding of Physical Fitness Effects",
    "section": "3.3 Other topics",
    "text": "3.3 Other topics\n\n3.3.1 Contrasts are re-parameterizations of the same model\nThe choice of contrast does not affect the model objective, in other words, they all yield the same goodness of fit. It does not matter whether a contrast is orthogonal or not.\n\n[\n  objective(m_ovi_SeqDiff),\n  objective(m_ovi_Helmert),\n  objective(m_ovi_Hypo),\n]\n\n3-element Vector{Float64}:\n 13786.139011987034\n 13786.139011986992\n 13786.139011986992\n\n\n\n\n3.3.2 VCs and CPs depend on contrast coding\nTrivially, the meaning of a contrast depends on its definition. Consequently, the contrast specification has a big effect on the random-effect structure. As an illustration, we refit the LMMs with variance components (VCs) and correlation parameters (CPs) for Child-related contrasts of Test. Unfortunately, it is not easy, actually rather quite difficult, to grasp the meaning of correlations of contrast-based effects; they represent two-way interactions.\n\nbegin\n  f_Child = @formula zScore ~\n    1 + Test * a1 * Sex + (1 + Test | Child)\n  m_Child_SDC = fit(MixedModel, f_Child, dat; contrasts=contr1)\n  m_Child_HeC = fit(MixedModel, f_Child, dat; contrasts=contr2)\n  m_Child_HyC = fit(MixedModel, f_Child, dat; contrasts=contr3)\n  m_Child_PCA = fit(MixedModel, f_Child, dat; contrasts=contr4)\nend\n\n\u001b[32mMinimizing 1557     Time: 0:00:06 ( 4.04 ms/it)\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0205\n0.0141\n-1.45\n0.1457\n0.7005\n\n\nTest: c5.1\n-0.0486\n0.0442\n-1.10\n0.2720\n1.1581\n\n\nTest: c234.15\n-0.0446\n0.1701\n-0.26\n0.7930\n0.7632\n\n\nTest: c2.34\n-0.0284\n0.0759\n-0.37\n0.7083\n1.7006\n\n\nTest: c3.4\n-0.0240\n0.0443\n-0.54\n0.5883\n1.4273\n\n\na1\n0.3008\n0.0477\n6.31\n<1e-09\n\n\n\nSex: Boys\n0.2281\n0.0141\n16.18\n<1e-58\n\n\n\nTest: c5.1 & a1\n0.6098\n0.1521\n4.01\n<1e-04\n\n\n\nTest: c234.15 & a1\n0.5428\n0.5788\n0.94\n0.3483\n\n\n\nTest: c2.34 & a1\n0.4829\n0.2531\n1.91\n0.0564\n\n\n\nTest: c3.4 & a1\n0.2752\n0.1481\n1.86\n0.0631\n\n\n\nTest: c5.1 & Sex: Boys\n0.0555\n0.0442\n1.26\n0.2091\n\n\n\nTest: c234.15 & Sex: Boys\n-1.0368\n0.1701\n-6.09\n<1e-08\n\n\n\nTest: c2.34 & Sex: Boys\n-0.0349\n0.0759\n-0.46\n0.6454\n\n\n\nTest: c3.4 & Sex: Boys\n-0.0634\n0.0443\n-1.43\n0.1528\n\n\n\na1 & Sex: Boys\n-0.0583\n0.0477\n-1.22\n0.2218\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n-0.0575\n0.1521\n-0.38\n0.7056\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n0.5697\n0.5788\n0.98\n0.3250\n\n\n\nTest: c2.34 & a1 & Sex: Boys\n-0.0274\n0.2531\n-0.11\n0.9139\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n-0.1126\n0.1481\n-0.76\n0.4471\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_SDC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.47600465\n0.68993090\n\n\n\n\n\n\n\nTest: Star_r\n0.45007735\n0.67087804\n-0.45\n\n\n\n\n\n\nTest: S20_r\n2.05159885\n1.43234034\n-0.03\n-0.85\n\n\n\n\n\nTest: SLJ\n0.69854966\n0.83579283\n+0.26\n+0.21\n-0.57\n\n\n\n\nTest: BPT\n1.29238582\n1.13683148\n-0.25\n+0.51\n-0.38\n-0.10\n\n\nResidual\n\n0.00000000\n0.00001739\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_HeC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.37170843\n0.60967896\n\n\n\n\n\n\n\nTest: Star_r\n0.22826701\n0.47777297\n-0.54\n\n\n\n\n\n\nTest: S20_r\n0.16381078\n0.40473545\n-0.10\n-0.79\n\n\n\n\n\nTest: SLJ\n0.05268477\n0.22953163\n+0.12\n+0.08\n-0.17\n\n\n\n\nTest: BPT\n0.03441157\n0.18550355\n-0.08\n-0.25\n+0.38\n+0.16\n\n\nResidual\n\n0.00000000\n0.00004676\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_HyC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.47843168\n0.69168756\n\n\n\n\n\n\n\nTest: BPT-other\n1.42983539\n1.19575725\n+0.96\n\n\n\n\n\n\nTest: Star-End\n1.46192294\n1.20910006\n-0.13\n+0.12\n\n\n\n\n\nTest: S20-Star\n1.47867101\n1.21600617\n+0.14\n+0.06\n-0.60\n\n\n\n\nTest: SLJ-S20\n1.41586176\n1.18989989\n-0.08\n-0.13\n-0.20\n-0.30\n\n\nResidual\n\n0.00000000\n0.00002981\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_Child_PCA)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\nChild\n(Intercept)\n0.49071876\n0.70051321\n\n\n\n\n\n\n\nTest: c5.1\n1.34123345\n1.15811634\n-0.04\n\n\n\n\n\n\nTest: c234.15\n0.58240598\n0.76315528\n-0.96\n-0.24\n\n\n\n\n\nTest: c2.34\n2.89197628\n1.70058116\n+0.24\n+0.31\n-0.32\n\n\n\n\nTest: c3.4\n2.03721883\n1.42731175\n+0.04\n-0.09\n-0.02\n+0.16\n\n\nResidual\n\n0.00000000\n0.00002443\n\n\n\n\n\n\n\n\n\nThe CPs for the various contrasts are in line with expectations. For the SDC we observe substantial negative CPs between neighboring contrasts. For the orthogonal HeC, all CPs are small; they are uncorrelated. HyC contains some of the SDC contrasts and we observe again the negative CPs. The (roughly) PCA-based contrasts are small with one exception; there is a sizeable CP of +.41 between GM and the core of adjusted physical fitness (c234.15).\nDo these differences in CPs imply that we can move to zcpLMMs when we have orthogonal contrasts? We pursue this question with by refitting the four LMMs with zerocorr() and compare the goodness of fit.\n\nbegin\n  f_Child0 = @formula zScore ~\n    1 + Test * a1 * Sex + zerocorr(1 + Test | Child)\n  m_Child_SDC0 = fit(MixedModel, f_Child0, dat; contrasts=contr1)\n  m_Child_HeC0 = fit(MixedModel, f_Child0, dat; contrasts=contr2)\n  m_Child_HyC0 = fit(MixedModel, f_Child0, dat; contrasts=contr3)\n  m_Child_PCA0 = fit(MixedModel, f_Child0, dat; contrasts=contr4)\nend\n\n\u001b[32mMinimizing 459      Time: 0:00:01 ( 3.13 ms/it)\u001b[39m\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Child\n\n\n\n\n(Intercept)\n-0.0206\n0.0141\n-1.46\n0.1443\n0.6927\n\n\nTest: c5.1\n-0.0421\n0.0415\n-1.01\n0.3105\n1.1689\n\n\nTest: c234.15\n-0.0447\n0.1671\n-0.27\n0.7891\n0.0018\n\n\nTest: c2.34\n-0.0363\n0.0783\n-0.46\n0.6428\n2.0362\n\n\nTest: c3.4\n-0.0258\n0.0467\n-0.55\n0.5804\n1.3287\n\n\na1\n0.3012\n0.0477\n6.32\n<1e-09\n\n\n\nSex: Boys\n0.2277\n0.0141\n16.13\n<1e-57\n\n\n\nTest: c5.1 & a1\n0.5960\n0.1431\n4.16\n<1e-04\n\n\n\nTest: c234.15 & a1\n0.5394\n0.5686\n0.95\n0.3428\n\n\n\nTest: c2.34 & a1\n0.4945\n0.2612\n1.89\n0.0583\n\n\n\nTest: c3.4 & a1\n0.2716\n0.1561\n1.74\n0.0818\n\n\n\nTest: c5.1 & Sex: Boys\n0.0530\n0.0415\n1.28\n0.2015\n\n\n\nTest: c234.15 & Sex: Boys\n-0.9992\n0.1671\n-5.98\n<1e-08\n\n\n\nTest: c2.34 & Sex: Boys\n-0.0291\n0.0783\n-0.37\n0.7101\n\n\n\nTest: c3.4 & Sex: Boys\n-0.0592\n0.0467\n-1.27\n0.2055\n\n\n\na1 & Sex: Boys\n-0.0581\n0.0477\n-1.22\n0.2228\n\n\n\nTest: c5.1 & a1 & Sex: Boys\n-0.0747\n0.1431\n-0.52\n0.6016\n\n\n\nTest: c234.15 & a1 & Sex: Boys\n0.6591\n0.5686\n1.16\n0.2464\n\n\n\nTest: c2.34 & a1 & Sex: Boys\n0.0193\n0.2612\n0.07\n0.9412\n\n\n\nTest: c3.4 & a1 & Sex: Boys\n-0.1157\n0.1561\n-0.74\n0.4585\n\n\n\nResidual\n0.0000\n\n\n\n\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_Child_SDC0, m_Child_SDC)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(>χ²)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + MixedModels.ZeroCorr((1 + Test | Child))\n26\n13784\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13318\n466\n10\n<1e-93\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_Child_HeC0, m_Child_HeC)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(>χ²)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + MixedModels.ZeroCorr((1 + Test | Child))\n26\n13304\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13351\n-47\n10\nNaN\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_Child_HyC0, m_Child_HyC)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(>χ²)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + MixedModels.ZeroCorr((1 + Test | Child))\n26\n13319\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13349\n-29\n10\nNaN\n\n\n\n\n\n\nMixedModels.likelihoodratiotest(m_Child_PCA0, m_Child_PCA)\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel-dof\ndeviance\nχ²\nχ²-dof\nP(>χ²)\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + MixedModels.ZeroCorr((1 + Test | Child))\n26\n13239\n\n\n\n\n\nzScore ~ 1 + Test + a1 + Sex + Test & a1 + Test & Sex + a1 & Sex + Test & a1 & Sex + (1 + Test | Child)\n36\n13347\n-108\n10\nNaN\n\n\n\n\n\nRK: The above results are not quite in line with what I obtained with earlier runs of this code, I think. This still needs some follow up.\nObviously, we can not drop CPs from any of the LMMs. The full LMMs all have the same objective, but we can compare the goodness-of-fit statistics of zcpLMMs more directly.\n\nbegin\n  zcpLMM = [\"SDC0\", \"HeC0\", \"HyC0\", \"PCA0\"]\n  mods = [m_Child_SDC0, m_Child_HeC0, m_Child_HyC0, m_Child_PCA0]\n  gof_summary = sort!(\n    DataFrame(;\n      zcpLMM=zcpLMM,\n      dof=dof.(mods),\n      deviance=deviance.(mods),\n      AIC=aic.(mods),\n      BIC=bic.(mods),\n    ),\n    :deviance,\n  )\nend\n\n4 rows × 5 columnszcpLMMdofdevianceAICBICStringInt64Float64Float64Float641PCA02613238.913290.913460.42HeC02613304.313356.313525.73HyC02613319.413371.413540.94SDC02613783.513835.514005.0\n\n\nRK: The following description is from an old version of the script; it does not match the current results.\nThe best fit was obtained for the PCA-based zcpLMM. Somewhat surprisingly the second best fit was obtained for the SDC. The relatively poor performance of HeC-based zcpLMM is puzzling to me. I thought it might be related to imbalance in design in the present data, but this does not appear to be the case. The same comparison of SequentialDifferenceCoding and Helmert Coding also showed a worse fit for the zcp-HeC LMM than the zcp-SDC LMM.\n\n\n3.3.3 VCs and CPs depend on random factor\nVCs and CPs resulting from a set of test contrasts can also be estimated for the random factor School. Of course, these VCs and CPs may look different from the ones we just estimated for Child.\nThe effect of age (i.e., developmental gain) varies within School. Therefore, we also include its VCs and CPs in this model; the school-related VC for Sex was not significant.\n\nf_School = @formula zScore ~\n  1 + Test * a1 * Sex + (1 + Test + a1 | School);\nm_School_SeqDiff = fit(MixedModel, f_School, dat; contrasts=contr1);\nm_School_Helmert = fit(MixedModel, f_School, dat; contrasts=contr2);\nm_School_Hypo = fit(MixedModel, f_School, dat; contrasts=contr3);\nm_School_PCA = fit(MixedModel, f_School, dat; contrasts=contr4);\n\n\u001b[32mMinimizing 998      Time: 0:00:00 ( 0.50 ms/it)\u001b[39m\n\n\n\nVarCorr(m_School_SeqDiff)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.045249\n0.212719\n\n\n\n\n\n\n\n\nTest: Star_r\n0.081325\n0.285176\n+0.17\n\n\n\n\n\n\n\nTest: S20_r\n0.151231\n0.388885\n-0.21\n-0.60\n\n\n\n\n\n\nTest: SLJ\n0.103060\n0.321029\n+0.09\n-0.42\n-0.39\n\n\n\n\n\nTest: BPT\n0.111357\n0.333701\n-0.10\n+0.32\n+0.09\n-0.79\n\n\n\n\na1\n0.057105\n0.238966\n+0.15\n-0.55\n-0.06\n+0.36\n+0.20\n\n\nResidual\n\n0.840758\n0.916929\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_School_Helmert)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.0452499\n0.2127202\n\n\n\n\n\n\n\n\nTest: Star_r\n0.0203289\n0.1425794\n+0.17\n\n\n\n\n\n\n\nTest: S20_r\n0.0116780\n0.1080647\n-0.17\n-0.28\n\n\n\n\n\n\nTest: SLJ\n0.0037258\n0.0610396\n-0.04\n-0.80\n+0.03\n\n\n\n\n\nTest: BPT\n0.0017894\n0.0423015\n-0.19\n-0.19\n+0.41\n-0.43\n\n\n\n\na1\n0.0570344\n0.2388187\n+0.15\n-0.55\n-0.31\n+0.20\n+0.49\n\n\nResidual\n\n0.8407609\n0.9169301\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_School_Hypo)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.045257\n0.212738\n\n\n\n\n\n\n\n\nTest: BPT-other\n0.712052\n0.843832\n-0.19\n\n\n\n\n\n\n\nTest: Star-End\n0.081405\n0.285315\n+0.17\n-0.19\n\n\n\n\n\n\nTest: S20-Star\n0.151377\n0.389072\n-0.20\n+0.42\n-0.60\n\n\n\n\n\nTest: SLJ-S20\n0.103144\n0.321161\n+0.09\n-0.60\n-0.42\n-0.39\n\n\n\n\na1\n0.056987\n0.238719\n+0.15\n+0.49\n-0.55\n-0.06\n+0.36\n\n\nResidual\n\n0.840774\n0.916937\n\n\n\n\n\n\n\n\n\n\n\nVarCorr(m_School_PCA)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\n\n\nSchool\n(Intercept)\n0.045258\n0.212738\n\n\n\n\n\n\n\n\nTest: c5.1\n0.054348\n0.233127\n-0.16\n\n\n\n\n\n\n\nTest: c234.15\n0.797643\n0.893109\n+0.16\n-0.34\n\n\n\n\n\n\nTest: c2.34\n0.515172\n0.717755\n+0.18\n-0.16\n+0.13\n\n\n\n\n\nTest: c3.4\n0.103070\n0.321046\n-0.09\n+0.91\n+0.05\n+0.03\n\n\n\n\na1\n0.057132\n0.239024\n+0.15\n+0.02\n-0.91\n-0.10\n-0.36\n\n\nResidual\n\n0.840761\n0.916930\n\n\n\n\n\n\n\n\n\n\nWe compare again how much of the fit resides in the CPs.\n\nbegin\n  f_School0 = @formula zScore ~\n    1 + Test * a1 * Sex + zerocorr(1 + Test + a1 | School)\n  m_School_SDC0 = fit(MixedModel, f_School0, dat; contrasts=contr1)\n  m_School_HeC0 = fit(MixedModel, f_School0, dat; contrasts=contr2)\n  m_School_HyC0 = fit(MixedModel, f_School0, dat; contrasts=contr3)\n  m_School_PCA0 = fit(MixedModel, f_School0, dat; contrasts=contr4)\n  #     \n  zcpLMM2 = [\"SDC0\", \"HeC0\", \"HyC0\", \"PCA0\"]\n  mods2 = [\n    m_School_SDC0, m_School_HeC0, m_School_HyC0, m_School_PCA0\n  ]\n  gof_summary2 = sort!(\n    DataFrame(;\n      zcpLMM=zcpLMM2,\n      dof=dof.(mods2),\n      deviance=deviance.(mods2),\n      AIC=aic.(mods2),\n      BIC=bic.(mods2),\n    ),\n    :deviance,\n  )\nend\n\n\u001b[32mMinimizing 307      Time: 0:00:00 ( 0.39 ms/it)\u001b[39m\n\u001b[34m  objective:  13755.58706721255\u001b[39m\n\n\n4 rows × 5 columnszcpLMMdofdevianceAICBICStringInt64Float64Float64Float641PCA02713755.613809.613985.62HeC02713759.113813.113989.13HyC02713763.613817.613993.54SDC02713763.913817.913993.9\n\n\nFor the random factor School the Helmert contrast, followed by PCA-based contrasts have least information in the CPs; SDC has the largest contribution from CPs. Interesting."
  },
  {
    "objectID": "contrasts_fggk21.html#thats-it",
    "href": "contrasts_fggk21.html#thats-it",
    "title": "3  Contrast Coding of Physical Fitness Effects",
    "section": "3.4 That’s it",
    "text": "3.4 That’s it\nThat’s it for this vignette. It is time to try your own contrast coding. You can use these data; there are many alternatives to set up hypotheses for the five tests. Of course and even better, code up some contrasts for data of your own.\nHave fun!\n\n\nFühner, T., Granacher, U., Golle, K., & Kliegl, R. (2021). Age and sex effects in physical fitness components of 108,295 third graders including 515 primary schools and 9 cohorts. Scientific Reports, 11(1). https://doi.org/10.1038/s41598-021-97000-4"
  },
  {
    "objectID": "kwdyz11.html",
    "href": "kwdyz11.html",
    "title": "4  RePsychLing RePsychLing Kliegl et al. (2011)",
    "section": "",
    "text": "We take the kwdyz11.arrow dataset (Kliegl et al., 2010) from an experiment looking at three effects of visual cueing under four different cue-target relations (CTRs). Two horizontal rectangles are displayed above and below a central fixation point or they displayed in vertical orientation to the left and right of the fixation point. Subjects react to the onset of a small visual target occuring at one of the four ends of the two rectangles. The target is cued validly on 70% of trials by a brief flash of the corner of the rectangle at which it appears; it is cued invalidly at the three other locations 10% of the trials each.\nWe specify three contrasts for the four-level factor CTR that are derived from spatial, object-based, and attractor-like features of attention. They map onto sequential differences between appropriately ordered factor levels. At the level of fixed effects, there is the noteworthy result, that the attraction effect was estimated at 2 ms, that is clearly not significant. Nevertheless, there was a highly reliable variance component (VC) estimated for this effect. Moreover, the reliable individual differences in the attraction effect were negatively correlated with those in the spatial effect.\nUnfortunately, a few years after the publication, we determined that the reported LMM is actually singular and that the singularity is linked to a theoretically critical correlation parameter (CP) between the spatial effect and the attraction effect. Fortunately, there is also a larger dataset kkl15.arrow from a replication and extension of this study (Kliegl et al., 2015), analyzed with kkl15.jl notebook. The critical CP (along with other fixed effects and CPs) was replicated in this study.\nA more comprehensive analysis was reported in the parsimonious mixed-model paper (Bates et al., 2015). Data and R scripts are also available in R-package RePsychLing. In this and the complementary kkl15.jl scripts, we provide some corresponding analyses with MixedModels.jl."
  },
  {
    "objectID": "kwdyz11.html#packages",
    "href": "kwdyz11.html#packages",
    "title": "4  RePsychLing RePsychLing Kliegl et al. (2011)",
    "section": "4.2 Packages",
    "text": "4.2 Packages\n\n\nCode\nusing Arrow\nusing AlgebraOfGraphics\nusing CairoMakie\nusing CategoricalArrays\nusing Chain\nusing DataFrames\nusing DataFrameMacros\nusing MixedModels\nusing MixedModelsMakie\nif contains(first(Sys.cpu_info()).model, \"Intel\")\n  using MKL             # faster LAPACK on Intel processors\nend\nusing ProgressMeter\nusing Random\nusing StatsBase\nusing Statistics\nusing AlgebraOfGraphics: density\nusing AlgebraOfGraphics: boxplot\n\nCairoMakie.activate!(; type=\"svg\")\nProgressMeter.ijulia_behavior(:clear);"
  },
  {
    "objectID": "kwdyz11.html#read-data-compute-and-plot-densities-and-means",
    "href": "kwdyz11.html#read-data-compute-and-plot-densities-and-means",
    "title": "4  RePsychLing RePsychLing Kliegl et al. (2011)",
    "section": "4.3 Read data, compute and plot densities and means",
    "text": "4.3 Read data, compute and plot densities and means\n\n\nCode\ndat = @chain \"./data/kwdyz11.arrow\" begin\n  Arrow.Table\n  DataFrame\n  select(\n    :subj =>\n      (s -> categorical(string.('S', lpad.(s, 2, '0')))) => :Subj,\n    :tar => categorical => :CTR,\n    :rt,\n    :rt => (x -> log.(x)) => :lrt,\n  )\nend\nlevels!(dat.CTR, [\"val\", \"sod\", \"dos\", \"dod\"])\ndescribe(dat)\n\n\n4 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1SubjS01S610CategoricalValue{String, UInt32}2CTRvaldod0CategoricalValue{String, UInt32}3rt370.426150.1358.6705.70Float644lrt5.8865.01135.882216.559190Float64\n\n\nWe recommend to code the levels/units of random factor / grouping variable not as a number, but as a string starting with a letter and of the same length for all levels/units.\nWe also recommend to sort levels of factors into a meaningful order, that is overwrite the default alphabetic ordering. This is also a good place to choose alternative names for variables in the context of the present analysis.\nThe LMM analysis is based on log-transformed reaction times lrt, indicated by a boxcox() check of model residuals. With the exception of diagnostic plots of model residuals, the analysis of untransformed reaction times did not lead to different results and exhibited the same problems of model identification (see Kliegl et al., 2010).\nComparative density plots of all response times by cue-target relation, Figure 4.1, show the times for valid cues to be faster than for the other conditions.\n\n\nCode\ndraw(\n  data(dat) *\n  mapping(\n    :lrt => \"log(Reaction time [ms])\";\n    color=:CTR =>\n      renamer(\"val\" => \"valid cue\", \"sod\" => \"some obj/diff pos\", \"dos\" => \"diff obj/same pos\", \"dod\" => \"diff obj/diff pos\") => \"Cue-target relation\",\n  ) *\n  density(),\n)\n\n\n\n\n\nFigure 4.1: Comparative density plots of log reaction time for different cue-target relations.\n\n\n\n\nAn alternative visualization without overlap of the conditions can be accomplished with ridge plots.\nTo be done\nFor the next set of plots we average subjects’ data within the four experimental conditions. This table could be used as input for a repeated-measures ANOVA.\n\ndat_subj = combine(\n  groupby(dat, [:Subj, :CTR]),\n  :rt => length => :n,\n  :rt => mean => :rt_m,\n  :lrt => mean => :lrt_m,\n)\n\n244 rows × 5 columnsSubjCTRnrt_mlrt_mCat…Cat…Int64Float64Float641S01val330413.3326.012722S01sod48437.7216.06823S01dos47443.3666.082694S01dod45434.3166.060795S02val333365.8995.87516S02sod47396.1495.959167S02dos46439.4876.069498S02dod48441.0426.068839S03val336371.4465.9048910S03sod46446.8546.0946411S03dos48471.3026.1428712S03dod47476.5326.1570613S04val336403.1815.9912414S04sod48446.0756.0949915S04dos48458.3046.1200516S04dod48441.0546.0820517S05val310358.7145.8411718S05sod44409.155.9493419S05dos45457.4936.1005320S05dod45472.1386.1336121S06val334362.8645.8795322S06sod48368.1235.8961723S06dos48383.0945.9360124S06dod48367.9735.8897925S07val326407.4975.9844726S07sod48459.06.1092727S07dos42486.5026.1772928S07dod47457.6176.1047129S08val333308.1025.7181530S08sod48323.8755.76388⋮⋮⋮⋮⋮⋮\n\n\n\n\nCode\nboxplot(\n  dat_subj.CTR.refs,\n  dat_subj.lrt_m;\n  orientation=:horizontal,\n  show_notch=true,\n  axis=(;\n    yticks=(\n      1:4,\n      [\n        \"valid cue\",\n        \"same obj/diff pos\",\n        \"diff obj/same pos\",\n        \"diff obj/diff pos\",\n      ],\n    ),\n  ),\n  figure=(; resolution=(800, 300)),\n)\n\n\n\nFigureAxisPlot()\nFigure 4.2: Comparative boxplots of log response time by cue-target relation.\n\n\n\nMean of log reaction times for four cue-target relations. Targets appeared at (a) the cued position (valid) in a rectangle, (b) in the same rectangle cue, but at its other end, (c) on the second rectangle, but at a corresponding horizontal/vertical physical distance, or (d) at the other end of the second rectangle, that is \\(\\sqrt{2}\\) of horizontal/vertical distance diagonally across from the cue, that is also at larger physical distance compared to (c).\nA better alternative to the boxplot is a dotplot. It also displays subjects’ condition means.\nTo be done"
  },
  {
    "objectID": "kwdyz11.html#linear-mixed-model",
    "href": "kwdyz11.html#linear-mixed-model",
    "title": "4  RePsychLing RePsychLing Kliegl et al. (2011)",
    "section": "4.4 Linear mixed model",
    "text": "4.4 Linear mixed model\n\ncontrasts = Dict(\n  :CTR => SeqDiffCoding(; levels=[\"val\", \"sod\", \"dos\", \"dod\"]),\n  :Subj => Grouping(),\n)\nm1 = let\n  form = @formula(lrt ~ 1 + CTR + (1 + CTR | Subj))\n  fit(MixedModel, form, dat; contrasts)\nend\n\n\u001b[32mMinimizing 242      Time: 0:00:00 ( 1.12 ms/it)\u001b[39m\n\u001b[34m  objective:  -12782.373738189599\u001b[39m\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n5.9358\n0.0185\n320.53\n<1e-99\n0.1441\n\n\nCTR: sod\n0.0878\n0.0084\n10.48\n<1e-24\n0.0582\n\n\nCTR: dos\n0.0366\n0.0062\n5.92\n<1e-08\n0.0274\n\n\nCTR: dod\n-0.0086\n0.0060\n-1.43\n0.1515\n0.0249\n\n\nResidual\n0.1920\n\n\n\n\n\n\n\n\n\n\nVarCorr(m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nSubj\n(Intercept)\n0.0207652\n0.1441015\n\n\n\n\n\n\nCTR: sod\n0.0033852\n0.0581828\n+0.48\n\n\n\n\n\nCTR: dos\n0.0007532\n0.0274453\n-0.24\n-0.15\n\n\n\n\nCTR: dod\n0.0006223\n0.0249461\n+0.30\n+0.93\n-0.43\n\n\nResidual\n\n0.0368543\n0.1919748\n\n\n\n\n\n\n\n\n\nissingular(m1)\n\ntrue\n\n\nLMM m1 is not fully supported by the data; it is overparameterized. This is also visible in the PCA: only three, not four PCS are needed to account for all the variance and covariance in the random-effect structure. The problem is the +.93 CP for spatial sod and attraction dod effects.\n\nfirst(MixedModels.PCA(m1))\n\n\nPrincipal components based on correlation matrix\n (Intercept)   1.0     .      .      .\n CTR: sod      0.48   1.0     .      .\n CTR: dos     -0.24  -0.15   1.0     .\n CTR: dod      0.3    0.93  -0.43   1.0\n\nNormalized cumulative variances:\n[0.5887, 0.8096, 1.0, 1.0]\n\nComponent loadings\n                PC1   PC2    PC3    PC4\n (Intercept)  -0.4   0.04   0.9    0.17\n CTR: sod     -0.6   0.4   -0.16  -0.68\n CTR: dos      0.33  0.91   0.06   0.23\n CTR: dod     -0.61  0.08  -0.41   0.68"
  },
  {
    "objectID": "kwdyz11.html#diagnostic-plots-of-lmm-residuals",
    "href": "kwdyz11.html#diagnostic-plots-of-lmm-residuals",
    "title": "4  RePsychLing RePsychLing Kliegl et al. (2011)",
    "section": "4.5 Diagnostic plots of LMM residuals",
    "text": "4.5 Diagnostic plots of LMM residuals\nDo model residuals meet LMM assumptions? Classic plots are\n\nResidual over fitted\nQuantiles of model residuals over theoretical quantiles of normal distribution\n\n\n4.5.1 Residual-over-fitted plot\nThe slant in residuals show a lower and upper boundary of reaction times, that is we have have too few short and too few long residuals. Not ideal, but at least width of the residual band looks similar across the fitted values, that is there is no evidence for heteroskedasticity.\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\nset_aog_theme!()\ndraw(\n  data((; f=fitted(m1), r=residuals(m1))) *\n  mapping(:f => \"Fitted values\", :r => \"Residual from model m1\") *\n  visual(Scatter);\n)\n\n\n\n\n\nFigure 4.3: Residuals versus the fitted values for model m1 of the log response time.\n\n\n\n\nWith many observations the scatterplot is not that informative. Contour plots or heatmaps may be an alternative.\n\n\nCode\nCairoMakie.activate!(; type=\"png\")\ndraw(\n  data((; f=fitted(m1), r=residuals(m1))) *\n  mapping(\n    :f => \"Fitted log response time\", :r => \"Residual from model m1\"\n  ) *\n  density();\n)\n\n\n\nFigureGrid()\nFigure 4.4: Heatmap of residuals versus fitted values for model m1\n\n\n\n\n\n4.5.2 Q-Q plot\nThe plot of quantiles of model residuals over corresponding quantiles of the normal distribution should yield a straight line along the main diagonal.\n\nqqnorm(residuals(m1); qqline=:none)\n\n\n\n\n\n\n4.5.3 Observed and theoretical normal distribution\nThe violation of expectation is again due to the fact that the distribution of residuals is much narrower than expected from a normal distribution, as shown in Figure 4.5. Overall, it does not look too bad.\n\n\nCode\nlet\n  n = nrow(dat)\n  dat_rz = DataFrame(;\n    value=vcat(residuals(m1) ./ std(residuals(m1)), randn(n)),\n    curve=vcat(fill.(\"residual\", n), fill.(\"normal\", n)),\n  )\n  draw(\n    data(dat_rz) *\n    mapping(:value => \"Standardized residuals\"; color=:curve) *\n    density(; bandwidth=0.1);\n  )\nend\n\n\n\n\n\nFigure 4.5: Kernel density plot of the standardized residuals from model m1 compared to a Gaussian"
  },
  {
    "objectID": "kwdyz11.html#conditional-modes",
    "href": "kwdyz11.html#conditional-modes",
    "title": "4  RePsychLing RePsychLing Kliegl et al. (2011)",
    "section": "4.6 Conditional modes",
    "text": "4.6 Conditional modes\nNow we move on to visualizations that are based on model parameters and subjects’ data, that is “predictions” of the LMM for subject’s GM and experimental effects. Three important plots are\n\nOverlay\nCaterpillar\nShrinkage\n\n\n4.6.1 Overlay\nThe first plot overlays shrinkage-corrected conditional modes of the random effects with within-subject-based and pooled GMs and experimental effects.\nTo be done\n\n\n4.6.2 Caterpillar plot\nThe caterpillar plot, Figure 4.6, also reveals the high correlation between spatial sod and attraction dod effects.\n\n\nCode\ncaterpillar!(\n  Figure(; resolution=(800, 1000)), ranefinfo(m1, :Subj); orderby=2\n)\n\n\n\n\n\nFigure 4.6: Prediction intervals on the random effects for Subj in model m1\n\n\n\n\n\n\n4.6.3 Shrinkage plot\nFigure 4.7 provides more evidence for a problem with the visualization of the spatial sod and attraction dod CP. The corresponding panel illustrates an implosion of conditional modes.\n\n\nCode\nshrinkageplot!(Figure(; resolution=(1000, 1000)), m1)\n\n\n\n\n\nFigure 4.7: Shrinkage plot of the conditional means of the random effects for model m1"
  },
  {
    "objectID": "kwdyz11.html#parametric-bootstrap",
    "href": "kwdyz11.html#parametric-bootstrap",
    "title": "4  RePsychLing RePsychLing Kliegl et al. (2011)",
    "section": "4.7 Parametric bootstrap",
    "text": "4.7 Parametric bootstrap\nHere we\n\ngenerate a bootstrap sample\ncompute shortest covergage intervals for the LMM parameters\nplot densities of bootstrapped parameter estimates for residual, fixed effects, variance components, and correlation parameters\n\n\n4.7.1 Generate a bootstrap sample\nWe generate 2500 samples for the 15 model parameters (4 fixed effect, 4 VCs, 6 CPs, and 1 residual).\n\n\nCode\nRandom.seed!(1234321)\nsamp = parametricbootstrap(2500, m1; hide_progress=true)\ndat2 = DataFrame(samp.allpars)\nfirst(dat2, 10)\n\n\n10 rows × 5 columnsitertypegroupnamesvalueInt64StringString?String?Float6411βmissing(Intercept)5.9339221βmissingCTR: sod0.086421731βmissingCTR: dos0.048890141βmissingCTR: dod-0.012185751σSubj(Intercept)0.13294261σSubjCTR: sod0.049742171ρSubj(Intercept), CTR: sod0.60496181σSubjCTR: dos0.027888891ρSubj(Intercept), CTR: dos-0.254434101ρSubjCTR: sod, CTR: dos0.0548647\n\n\n\nnrow(dat2) # 2500 estimates for each of 15 model parameters \n\n37500\n\n\n\n\n4.7.2 Shortest coverage interval\nThe upper limit of the interval for the critical CP CTR: sod, CTR: dod is hitting the upper wall of a perfect correlation. This is evidence of singularity. The other intervals do not exhibit such pathologies; they appear to be ok.\n\n\nCode\nDataFrame(shortestcovint(samp))\n\n\n15 rows × 5 columnstypegroupnameslowerupperStringString?String?Float64Float641βmissing(Intercept)5.899175.972562βmissingCTR: sod0.07193010.1045893βmissingCTR: dos0.0251170.04916674βmissingCTR: dod-0.02071820.00268275σSubj(Intercept)0.1165630.168576σSubjCTR: sod0.04457340.06988617ρSubj(Intercept), CTR: sod0.2436750.7129958σSubjCTR: dos0.009423230.04101379ρSubj(Intercept), CTR: dos-0.9999210.17008910ρSubjCTR: sod, CTR: dos-0.7269730.49290911σSubjCTR: dod0.01329210.037600612ρSubj(Intercept), CTR: dod-0.1325810.74056913ρSubjCTR: sod, CTR: dod0.573820.99999514ρSubjCTR: dos, CTR: dod-0.8947410.43564915σresidualmissing0.190510.19359\n\n\n\n\n4.7.3 Comparative density plots of bootstrapped parameter estimates\n\n4.7.3.1 Residual\n\n\nCode\ndraw(\n  data(@subset(dat2, :type == \"σ\", ismissing(:names))) *\n  mapping(:value => \"Residual standard deviation\") *\n  density();\n)\n\n\n\n\n\nFigure 4.8: ?(caption)\n\n\n\n\n\n\n4.7.3.2 Fixed effects (w/o GM)\nThe shortest coverage interval for the GM ranges from 376 to 404 ms. To keep the plot range small we do not inlcude its density here.\n\n\nCode\nlabels = [\n  \"CTR: sod\" => \"spatial effect\",\n  \"CTR: dos\" => \"object effect\",\n  \"CTR: dod\" => \"attraction effect\",\n  \"(Intercept)\" => \"grand mean\",\n]\ndraw(\n  data(@subset(dat2, :type == \"β\" && :names ≠ \"(Intercept)\")) *\n  mapping(\n    :value => \"Experimental effect size [ms]\";\n    color=:names => renamer(labels) => \"Experimental effects\",\n  ) *\n  density();\n)\n\n\n\n\n\nFigure 4.9: Comparative density plots of the fixed-effects parameters for model m1\n\n\n\n\nThe densitiies correspond nicely with the shortest coverage intervals.\n\n\n4.7.3.3 Variance components (VCs)\n\n\nCode\ndraw(\n  data(@subset(dat2, :type == \"σ\" && :group == \"Subj\")) *\n  mapping(\n    :value => \"Standard deviations [ms]\";\n    color=:names => renamer(labels) => \"Variance components\",\n  ) *\n  density();\n)\n\n\n\n\n\nFigure 4.10: Comparative density plots of the variance components for model m1\n\n\n\n\nThe VC are all very nicely defined.\n\n\n4.7.3.4 Correlation parameters (CPs)\n\n\nCode\nlet\n  labels = [\n    \"(Intercept), CTR: sod\" => \"GM, spatial\",\n    \"(Intercept), CTR: dos\" => \"GM, object\",\n    \"CTR: sod, CTR: dos\" => \"spatial, object\",\n    \"(Intercept), CTR: dod\" => \"GM, attraction\",\n    \"CTR: sod, CTR: dod\" => \"spatial, attraction\",\n    \"CTR: dos, CTR: dod\" => \"object, attraction\",\n  ]\n  draw(\n    data(@subset(dat2, :type == \"ρ\")) *\n    mapping(\n      :value => \"Correlation\";\n      color=:names => renamer(labels) => \"Correlation parameters\",\n    ) *\n    density();\n  )\nend\n\n\n\n\n\nFigure 4.11: Comparative density plots of the correlation parameters for model m1\n\n\n\n\nTwo of the CPs stand out positively. First, the correlation between GM and the spatial effect is well defined. Second, as discussed throughout this script, the CP between spatial and attraction effect is close to the 1.0 border and clearly not well defined. Therefore, this CP will be replicated with a larger sample in script kkl15.jl (Kliegl et al., 2015).\n\n\nBates, D., Kliegl, R., Vasishth, S., & Baayen, H. (2015). Parsimonius mixed models. arXiv, 1506.04967v1.\n\n\nKliegl, R., Kushela, J., & Laubrock, J. (2015). Object orientation and target size modulate the speed of visual attention. Department of Psychology, University of Potsdam.\n\n\nKliegl, R., Wei, P., Dambacher, M., Yan, M., & Zhou, X. (2010). Experimental effects and individual differences in linear mixed models: Estimating the relationship between spatial, object, and attraction effects in visual attention. Frontiers in Psychology. https://doi.org/10.3389/fpsyg.2010.00238"
  },
  {
    "objectID": "kkl15.html",
    "href": "kkl15.html",
    "title": "5  RePsychLing RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "",
    "text": "Kliegl et al. (2015) is a follow-up to Kliegl et al. (2010) (see also script contrasts_kwdyz11.qmd) from an experiment looking at a variety of effects of visual cueing under four different cue-target relations (CTRs). In this experiment two rectangles are displayed (1) in horizontal orientation , (2) in vertical orientation, (3) in left diagonal orientation, or in (4) right diagonal orientation relative to a central fixation point. Subjects react to the onset of a small or a large visual target occuring at one of the four ends of the two rectangles. The target is cued validly on 70% of trials by a brief flash of the corner of the rectangle at which it appears; it is cued invalidly at the three other locations 10% of the trials each. This implies a latent imbalance in design that is not visiable in the repeated-measures ANOVA, but we will show its effect in the random-effect structure and conditional modes.\nThere are a couple of differences between the first and this follow-up experiment, rendering it more a conceptual than a direct replication. First, the original experiment was carried out at Peking University and this follow-up at Potsdam University. Second, diagonal orientations of rectangles and large target sizes were not part of the design of Kliegl et al. (2010). To keep matters somewhat simpler and comparable we ignore them in this script.\nWe specify three contrasts for the four-level factor CTR that are derived from spatial, object-based, and attractor-like features of attention. They map onto sequential differences between appropriately ordered factor levels. Replicating Kliegl et al. (2010), the attraction effect was not significant as a fixed effect, but yielded a highly reliable variance component (VC; i.e., reliable individual differences in positive and negative attraction effects cancel the fixed effect). Moreover, these individual differences in the attraction effect were negatively correlated with those in the spatial effect.\nThis comparison is of interest because a few years after the publication of Kliegl et al. (2010), the theoretically critical correlation parameter (CP) between the spatial effect and the attraction effect was determined as the source of a non-singular LMM in that paper. The present study served the purpose to estimate this parameter with a larger sample and a wider variety of experimental conditions. Therefore, the code in this script is largely the same as the one in kwdyz.jl.\nThere will be another vignette modelling the additional experimental manipulations of target size and orientation of cue rectangle. This analysis was reported in the parsimonious mixed-model paper (Bates et al., 2015); they were also used in a paper of GAMMs (Baayen et al., 2017). Data and R scripts are also available in R-package RePsychLing. Here we provide some of the corresponding analyses with MixedModels.jl and a much wider variety of visualizations of LMM results. A MixedModels.jl-based analysis focusing on the complex experimental design and the analysis of a complex random-effect structure for this design is in script kkl15_complex.jl."
  },
  {
    "objectID": "kkl15.html#packages",
    "href": "kkl15.html#packages",
    "title": "5  RePsychLing RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "5.2 Packages",
    "text": "5.2 Packages\n\n\nCode\nusing Arrow\nusing AlgebraOfGraphics\nusing CairoMakie\nusing CategoricalArrays\nusing Chain\nusing DataFrameMacros\nusing DataFrames\nusing MixedModels\nusing MixedModelsMakie\nif contains(first(Sys.cpu_info()).model, \"Intel\")\n  using MKL             # faster LAPACK on Intel processors\nend\nusing Random\nusing ProgressMeter\nusing Statistics\nusing StatsBase\n\nusing AlgebraOfGraphics: density\nusing AlgebraOfGraphics: boxplot\nusing MixedModelsMakie: qqnorm\nusing MixedModelsMakie: ridgeplot\nusing MixedModelsMakie: scatter\nconst datadir = joinpath(@__DIR__, \"data\")\n\nProgressMeter.ijulia_behavior(:clear)\nCairoMakie.activate!(; type=\"png\")"
  },
  {
    "objectID": "kkl15.html#read-data-compute-and-plot-means",
    "href": "kkl15.html#read-data-compute-and-plot-means",
    "title": "5  RePsychLing RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "5.3 Read data, compute and plot means",
    "text": "5.3 Read data, compute and plot means\n\ndat = @chain \"kkl15.arrow\" begin\n  joinpath(datadir, _)\n  Arrow.Table\n  DataFrame\n  select(\n    :subj =>\n      (s -> categorical(string.('S', lpad.(s, 3, '0')))) => :Subj,\n    :tar => categorical => :CTR,\n    :rt => (x -> log.(x)) => :lrt,\n    :rt,\n  )\nend\nlevels!(dat.CTR, [\"val\", \"sod\", \"dos\", \"dod\"])\ndescribe(dat)\n\n4 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1SubjS001S1470CategoricalValue{String, UInt32}2CTRvaldod0CategoricalValue{String, UInt32}3lrt5.644245.01215.622556.619380Float644rt293.147150.22276.594749.4810Float64\n\n\nWe recommend to code the levels/units of random factor / grouping variable not as a number, but as a string starting with a letter and of the same length for all levels/units.\nWe also recommend to sort levels of factors into a meaningful order, that is overwrite the default alphabetic ordering. This is also a good place to choose alternative names for variables in the context of the present analysis.\nThe LMM analysis is based on log-transformed reaction times lrt, indicated by a boxcox() check of model residuals. With the exception of diagnostic plots of model residuals, the analysis of untransformed reaction times did not lead to different results.\nComparative density plots of all response times by cue-target relation show the times for valid cues to be faster than for the other conditions.\n\n\nCode\ndraw(\n  data(dat) *\n  mapping(\n    :lrt => \"log(Reaction time [ms])\";\n    color=:CTR =>\n      renamer(\"val\" => \"valid cue\", \"sod\" => \"some obj/diff pos\", \"dos\" => \"diff obj/same pos\", \"dod\" => \"diff obj/diff pos\") => \"Cue-target relation\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n\n\nFigure 5.1: Compartive density plots of log response time by condition\n\n\n\n\nBoxplots of the mean of log response time by subject under the different conditions show an outlier value under three of the four conditions; they are from the same subject.\n\ndat_subj = combine(\n  groupby(dat, [:Subj, :CTR]),\n  :rt => length => :n,\n  :rt => mean => :rt_m,\n  :lrt => mean => :lrt_m,\n)\ndescribe(dat_subj)\n\n5 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1SubjS001S1470CategoricalValue{String, UInt32}2CTRvaldod0CategoricalValue{String, UInt32}3n156.2944964.04480Int644rt_m308.223208.194304.862584.710Float645lrt_m5.69085.332265.698486.361410Float64\n\n\n\n\nCode\nboxplot(\n  dat_subj.CTR.refs,\n  dat_subj.lrt_m;\n  orientation=:horizontal,\n  show_notch=true,\n  axis=(;\n    yticks=(\n      1:4,\n      [\n        \"valid cue\",\n        \"same obj/diff pos\",\n        \"diff obj/same pos\",\n        \"diff obj/diff pos\",\n      ],\n    ),\n  ),\n  figure=(; resolution=(800, 300)),\n)\n\n\n\nFigureAxisPlot()\nFigure 5.2: Comparative boxplots of mean response by subject under different conditions\n\n\n\nMean of log reaction times for four cue-target relations. Targets appeared at (a) the cued position (valid) in a rectangle, (b) in the same rectangle cue, but at its other end, (c) on the second rectangle, but at a corresponding horizontal/vertical physical distance, or (d) at the other end of the second rectangle, that is \\(\\sqrt{2}\\) of horizontal/vertical distance diagonally across from the cue, that is also at larger physical distance compared to (c).\nWe remove the outlier subject and replot, but we model the data points in dat and check whether this subject appears as an outlier in the caterpillar plot of conditional modes.\n\n\nCode\nlet\n  dat_subj2 = @subset(dat_subj, :rt_m < 510)\n  boxplot(\n    dat_subj2.CTR.refs,\n    dat_subj2.lrt_m;\n    orientation=:horizontal,\n    show_notch=true,\n    axis=(;\n      yticks=(\n        1:4,\n        [\n          \"valid cue\",\n          \"same obj/diff pos\",\n          \"diff obj/same pos\",\n          \"diff obj/diff pos\",\n        ],\n      ),\n    ),\n    figure=(; resolution=(800, 300)),\n  )\nend\n\n\n\nFigureAxisPlot()\nFigure 5.3: Comparative boxplots of mean response by subject under different conditions without outlier\n\n\n\nA better alternative to the boxplot is often a dotplot, because it also displays subjects’ condition means.\nTo be done\nFor the next set of plots we average subjects’ data within the four experimental conditions. This table could be used as input for a repeated-measures ANOVA.\n\ndat_cond = combine(\n  groupby(dat_subj, :CTR),\n  :n => length => :N,\n  :lrt_m => mean => :lrt_M,\n  :lrt_m => std => :lrt_SD,\n  :lrt_m => (x -> std(x) / sqrt(length(x))) => :lrt_SE,\n)\n\n4 rows × 5 columnsCTRNlrt_Mlrt_SDlrt_SECat…Int64Float64Float64Float641val865.614430.158050.01704292sod865.688360.1913950.02063863dos865.729430.1910270.02059894dod865.730990.216280.023322\n\n\nWe can also look at correlations plots based on the four condition means. There are actually two correlation matrices which have correspondences in alternative parameterizatios of the LMM random-effect structure. One matrix is based on the four measures. If you think of the four measures as test scores, this matrix is the usual correlation matrix. The second matrix contains correlations between the Grand Mean (GM) and the three effects defined with the contrasts for the four levels of the condition factor in the next chunk.\nTo this end, we\n\nuse the unstack() command to convert data from long to wide format,\ncompute the GM and the three experimental effects.\nplot the correlation matrix for four measures/scores, and\nplot the correlation matrix for GM and three effects\n\n\ndat_subj_w = @chain dat_subj begin\n  unstack(:Subj, :CTR, :rt_m)\n  disallowmissing!\n  @transform(\n    :GM = (:val + :sod + :dos + :dod) ./ 4,\n    :spatial = :sod - :val,\n    :object = :dos - :sod,\n    :attraction = :dod - :dos,\n  )\nend\ndescribe(dat_subj_w)\n\n9 rows × 7 columnsvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1SubjS001S1470CategoricalValue{String, UInt32}2val283.688216.35286.376513.530Float643sod306.75213.444305.092562.0150Float644dos319.896215.787317.527584.710Float645dod322.561208.194315.511555.2060Float646GM308.223217.819309.381553.8650Float647spatial23.0621-47.203620.227287.78730Float648object13.1456-13.827311.790361.04240Float649attraction2.66497-43.8703-1.1188763.53470Float64\n\n\n\n#@df dat_subj_w StatsPlots.corrplot(cols(2:5), grid = false, compact=false)\n\n\n#@df dat_subj_w StatsPlots.corrplot(cols(6:9), grid = false, compact=false)\n\n\n\n\n\n\n\nNote\n\n\n\nTwo of the theoreticsally irrelevant within-subject effect correlations have a different sign than the corresponding, non-significant CPs in the LMM; they are negative here, numerically positive in the LMM. This occurs only very rarely in the case of ecological correlations. However, as they are not significant according to shortest coverage interval, it may not be that relevant either. It is the case both for effects based on log-transformed and raw reaction times."
  },
  {
    "objectID": "kkl15.html#linear-mixed-model",
    "href": "kkl15.html#linear-mixed-model",
    "title": "5  RePsychLing RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "5.4 Linear mixed model",
    "text": "5.4 Linear mixed model\n\ncontrasts = Dict(\n  :Subj => Grouping(),\n  :CTR => SeqDiffCoding(; levels=[\"val\", \"sod\", \"dos\", \"dod\"]),\n)\nm1 = let\n  form = @formula lrt ~ 1 + CTR + (1 + CTR | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend\n\n\u001b[32mMinimizing 325      Time: 0:00:00 ( 0.76 ms/it)\u001b[39m\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n5.6907\n0.0199\n286.42\n<1e-99\n0.1839\n\n\nCTR: sod\n0.0740\n0.0080\n9.30\n<1e-19\n0.0688\n\n\nCTR: dos\n0.0409\n0.0038\n10.74\n<1e-26\n0.0011\n\n\nCTR: dod\n0.0016\n0.0057\n0.28\n0.7771\n0.0387\n\n\nResidual\n0.1971\n\n\n\n\n\n\n\n\n\n\nVarCorr(m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nSubj\n(Intercept)\n0.03382587\n0.18391810\n\n\n\n\n\n\nCTR: sod\n0.00472955\n0.06877175\n+0.56\n\n\n\n\n\nCTR: dos\n0.00000126\n0.00112042\n-0.05\n+0.80\n\n\n\n\nCTR: dod\n0.00149691\n0.03868996\n+0.60\n+0.66\n+0.36\n\n\nResidual\n\n0.03884575\n0.19709326\n\n\n\n\n\n\n\n\n\nissingular(m1)\n\ntrue\n\n\n\nonly(MixedModels.PCA(m1))\n\n\nPrincipal components based on correlation matrix\n (Intercept)   1.0     .      .      .\n CTR: sod      0.56   1.0     .      .\n CTR: dos     -0.05   0.8    1.0     .\n CTR: dod      0.6    0.66   0.36   1.0\n\nNormalized cumulative variances:\n[0.6325, 0.9136, 1.0, 1.0]\n\nComponent loadings\n                PC1    PC2    PC3    PC4\n (Intercept)  -0.41   0.66  -0.47   0.42\n CTR: sod     -0.6   -0.18  -0.34  -0.7\n CTR: dos     -0.43  -0.69  -0.07   0.58\n CTR: dod     -0.53   0.25   0.81  -0.0\n\n\nWe note that the critical correlation parameter between spatial (sod) and attraction (dod) is now estimated at .66 – not that close to the 1.0 boundary that caused singularity in Kliegl et al. (2010). However, the LMM based on log reaction times is still singular. Let’s check for untransformed reaction times.\n\nm1_rt = let\n  form = @formula rt ~ 1 + CTR + (1 + CTR | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n308.2059\n6.4152\n48.04\n<1e-99\n59.3789\n\n\nCTR: sod\n23.0720\n2.6416\n8.73\n<1e-17\n22.8512\n\n\nCTR: dos\n13.0855\n1.4585\n8.97\n<1e-18\n6.8351\n\n\nCTR: dod\n2.6860\n2.0608\n1.30\n0.1924\n15.1021\n\n\nResidual\n65.2246\n\n\n\n\n\n\n\n\n\n\nVarCorr(m1_rt)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nSubj\n(Intercept)\n3525.85438\n59.37891\n\n\n\n\n\n\nCTR: sod\n522.17688\n22.85119\n+0.66\n\n\n\n\n\nCTR: dos\n46.71799\n6.83506\n+0.35\n+0.15\n\n\n\n\nCTR: dod\n228.07278\n15.10208\n+0.53\n+0.65\n+0.30\n\n\nResidual\n\n4254.24728\n65.22459\n\n\n\n\n\n\n\n\n\nissingular(m1_rt)\n\nfalse\n\n\nFor untransformed reaction times, we see the model is not singular."
  },
  {
    "objectID": "kkl15.html#diagnostic-plots-of-lmm-residuals",
    "href": "kkl15.html#diagnostic-plots-of-lmm-residuals",
    "title": "5  RePsychLing RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "5.5 Diagnostic plots of LMM residuals",
    "text": "5.5 Diagnostic plots of LMM residuals\nDo model residuals meet LMM assumptions? Classic plots are\n\nResidual over fitted\nQuantiles of model residuals over theoretical quantiles of normal distribution\n\n\n5.5.1 Residual-over-fitted plot\nThe slant in residuals show a lower and upper boundary of reaction times, that is we have have too few short and too few long residuals. Not ideal, but at least width of the residual band looks similar across the fitted values, that is there is no evidence for heteroskedasticity.\n\n\nCode\nscatter(fitted(m1), residuals(m1))\n\n\n\n\n\nFigure 5.4: Residuals versus fitted values for model m1\n\n\n\n\nWith many observations the scatterplot is not that informative. Contour plots or heatmaps may be an alternative.\n\n\nCode\nset_aog_theme!()\ndraw(\n  data((; f=fitted(m1), r=residuals(m1))) *\n  mapping(\n    :f => \"Fitted values from m1\", :r => \"Residuals from m1\"\n  ) *\n  density();\n)\n\n\n\nFigureGrid()\nFigure 5.5: Heatmap of residuals versus fitted values for model m1\n\n\n\n\n\n5.5.2 Q-Q plot\nThe plot of quantiles of model residuals over corresponding quantiles of the normal distribution should yield a straight line along the main diagonal.\n\n\n\nCode\nqqnorm(m1; qqline=:none)\n\nFigure 5.6: ?(caption)\n\n\n\n\n\nCode\nqqnorm(m1_rt; qqline=:none)\n\nFigure 5.7: ?(caption)\n\n\n\n\n5.5.3 Observed and theoretical normal distribution\nThe violation of expectation is again due to the fact that the distribution of residuals is narrower than expected from a normal distribution. We can see this in this plot. Overall, it does not look too bad.\n\n\nCode\nlet\n  n = nrow(dat)\n  dat_rz = (;\n    value=vcat(residuals(m1) ./ std(residuals(m1)), randn(n)),\n    curve=repeat([\"residual\", \"normal\"]; inner=n),\n  )\n  draw(\n    data(dat_rz) *\n    mapping(:value; color=:curve) *\n    density(; bandwidth=0.1);\n  )\nend\n\n\n\n\n\nFigure 5.8: Kernel density plot of the standardized residuals for model m1 versus a standard normal"
  },
  {
    "objectID": "kkl15.html#conditional-modes",
    "href": "kkl15.html#conditional-modes",
    "title": "5  RePsychLing RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "5.6 Conditional modes",
    "text": "5.6 Conditional modes\n\n5.6.1 Caterpillar plot\n\n\nCode\ncm1 = only(ranefinfo(m1))\ncaterpillar!(Figure(; resolution=(800, 1200)), cm1; orderby=2)\n\n\n\n\n\nFigure 5.9: Prediction intervals of the subject random effects in model m1\n\n\n\n\nWhen we order the conditional modes for GM, that is (Intercept), the outlier subject S113 becomes visible; the associated experimental effects are not unusual.\n\n\nCode\ncaterpillar!(Figure(; resolution=(800, 1200)), cm1; orderby=1)\n\n\n\n\n\nFigure 5.10: Prediction intervals of the subject random effects in model m1 ordered by mean response\n\n\n\n\nThe catepillar plot also reveals that credibilty intervals are much shorter for subjects’ Grand Means, shown in (Intercept), than the subjects’ experimental effects, because the latter are based on difference scores not means. Moreover, credibility intervals are shorter for the first spatial effect sod than the other two effects, because the spatial effect involves the valid condition which yielded three times as many trials than the other three conditions. Consequently, the spatial effect is more reliable. Unfortunately, due to differences in scaling of the x-axis of the panels this effect must be inferred. One option to reveal this difference is to reparameterize the LMM such model parameters estimate the conditional modes for the levels of condition rather than the contrast-based effects. This is accomplished by replacing the 1 in the random effect term with 0, as shown next.\n\nm1L = let\n  form = @formula rt ~ 1 + CTR + (0 + CTR | Subj)\n  fit(MixedModel, form, dat; contrasts)\nend\n\n\n\n\n\nEst.\nSE\nz\np\nσ_Subj\n\n\n\n\n(Intercept)\n308.2059\n6.4199\n48.01\n<1e-99\n\n\n\nCTR: sod\n23.0720\n2.6415\n8.73\n<1e-17\n60.2203\n\n\nCTR: dos\n13.0855\n1.4589\n8.97\n<1e-18\n62.5228\n\n\nCTR: dod\n2.6860\n2.0617\n1.30\n0.1926\n71.5967\n\n\nCTR: val\n\n\n\n\n47.3219\n\n\nResidual\n65.2245\n\n\n\n\n\n\n\n\n\n\nVarCorr(m1L)\n\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\n\n\nSubj\nCTR: val\n2239.3599\n47.3219\n\n\n\n\n\n\nCTR: sod\n3626.4889\n60.2203\n+0.94\n\n\n\n\n\nCTR: dos\n3909.1032\n62.5228\n+0.94\n+0.99\n\n\n\n\nCTR: dod\n5126.0921\n71.5967\n+0.89\n+0.98\n+0.98\n\n\nResidual\n\n4254.2348\n65.2245\n\n\n\n\n\n\n\n\nThe caterpillar plot for levels shows the effect of the number of trials on credibility intervals; they are obviously much shorter for the valid condition. Note that this effect is not visible in a repeated-measure ANOVA with four condition means per subject as input.\n\n\nCode\n@chain m1L begin\n  ranefinfo\n  only\n  caterpillar!(Figure(; resolution=(800, 1000)), _; orderby=1)\nend\n\n\n\n\n\nFigure 5.11: Prediction intervals of the subject random effects in model m1L\n\n\n\n\n\n\n5.6.2 Shrinkage plot\n\n5.6.2.1 Log-transformed reaction times (LMM m1)\n\n\nCode\nshrinkageplot!(Figure(; resolution=(1000, 1200)), m1)\n\n\n\n\n\nFigure 5.12: Shrinkage plots of the subject random effects in model m1L\n\n\n\n\nThree of the CPs are imploded, but not the theoretically critical ones. These implosions did not occur (or were not as visible) for raw reaction times.\n\n\n5.6.2.2 Raw reaction times (LMM m1_rt)\n\n\nCode\nshrinkageplot!(Figure(; resolution=(1000, 1200)), m1_rt)\n\n\n\n\n\nFigure 5.13: Shrinkage plots of the subject random effects in model m1_rt\n\n\n\n\nThe implosion is for three CP visualizations is not observed for raw reaction times. Interesting."
  },
  {
    "objectID": "kkl15.html#parametric-bootstrap",
    "href": "kkl15.html#parametric-bootstrap",
    "title": "5  RePsychLing RePsychLing Kliegl, Kuschela, & Laubrock (2015)",
    "section": "5.7 Parametric bootstrap",
    "text": "5.7 Parametric bootstrap\nHere we\n\ngenerate a bootstrap sample\ncompute shortest covergage intervals for the LMM parameters\nplot densities of bootstrapped parameter estimates for residual, fixed effects, variance components, and correlation parameters\n\n\n5.7.1 Generate a bootstrap sample\nWe generate 2500 samples for the 15 model parameters (4 fixed effect, 4 VCs, 6 CPs, and 1 residual).\n\nRandom.seed!(1234321)\nsamp = parametricbootstrap(2500, m1);\n\n\ndat2 = DataFrame(samp.allpars)\nfirst(dat2, 10)\n\n10 rows × 5 columnsitertypegroupnamesvalueInt64StringString?String?Float6411βmissing(Intercept)5.691921βmissingCTR: sod0.077742931βmissingCTR: dos0.040677541βmissingCTR: dod-0.0033065751σSubj(Intercept)0.18253661σSubjCTR: sod0.062418271ρSubj(Intercept), CTR: sod0.67673381σSubjCTR: dos0.0037550691ρSubj(Intercept), CTR: dos-0.3046101ρSubjCTR: sod, CTR: dos0.49511\n\n\n\nnrow(dat2) # 2500 estimates for each of 15 model parameters \n\n37500\n\n\n\n\n5.7.2 Shortest coverage interval\n\nDataFrame(shortestcovint(samp))\n\n15 rows × 5 columnstypegroupnameslowerupperStringString?String?Float64Float641βmissing(Intercept)5.64875.727752βmissingCTR: sod0.05795010.08897373βmissingCTR: dos0.03291570.04770174βmissingCTR: dod-0.009836660.01273725σSubj(Intercept)0.1533550.2075686σSubjCTR: sod0.05619660.07954427ρSubj(Intercept), CTR: sod0.3881750.7167068σSubjCTR: dos0.0008167010.01783289ρSubj(Intercept), CTR: dos-0.909810.99994510ρSubjCTR: sod, CTR: dos-0.8795011.011σSubjCTR: dod0.02786250.048353712ρSubj(Intercept), CTR: dod0.3925020.81495813ρSubjCTR: sod, CTR: dod0.4372780.8854114ρSubjCTR: dos, CTR: dod-0.8632160.93059515σresidualmissing0.1958760.198207\n\n\nWe can also visualize the shortest coverage intervals for fixed effects with the ridgeplot() command:\n\n\nCode\nridgeplot(samp; show_intercept=false)\n\n\n\n\n\nFigure 5.14: Ridge plot of fixed-effects bootstrap samples from model m1L\n\n\n\n\n\n\n5.7.3 Comparative density plots of bootstrapped parameter estimates\n\n5.7.3.1 Residual\n\n\nCode\ndraw(\n  data(@subset(dat2, :type == \"σ\" && :group == \"residual\")) *\n  mapping(:value => \"Residual\") *\n  density();\n  figure=(; resolution=(800, 400)),\n)\n\n\n\n\n\nFigure 5.15: Kernel density estimate from bootstrap samples of the residual standard deviation for model m1L\n\n\n\n\n\n\n5.7.3.2 Fixed effects (w/o GM)\nThe shortest coverage interval for the GM ranges from 376 to 404 ms. To keep the plot range small we do not include its density here.\n\n\nCode\nrn = renamer([\n  \"(Intercept)\" => \"GM\",\n  \"CTR: sod\" => \"spatial effect\",\n  \"CTR: dos\" => \"object effect\",\n  \"CTR: dod\" => \"attraction effect\",\n  \"(Intercept), CTR: sod\" => \"GM, spatial\",\n  \"(Intercept), CTR: dos\" => \"GM, object\",\n  \"CTR: sod, CTR: dos\" => \"spatial, object\",\n  \"(Intercept), CTR: dod\" => \"GM, attraction\",\n  \"CTR: sod, CTR: dod\" => \"spatial, attraction\",\n  \"CTR: dos, CTR: dod\" => \"object, attraction\",\n])\ndraw(\n  data(@subset(dat2, :type == \"β\" && :names ≠ \"(Intercept)\")) *\n  mapping(\n    :value => \"Experimental effect size [ms]\";\n    color=:names => rn => \"Experimental effects\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n\n\nFigure 5.16: Kernel density estimate from bootstrap samples of the fixed effects for model m1L\n\n\n\n\nThe densitiies correspond nicely with the shortest coverage intervals.\n\n\nCode\ndraw(\n  data(@subset(dat2, :type == \"σ\" && :group == \"Subj\")) *\n  mapping(\n    :value => \"Standard deviations [ms]\";\n    color=:names => rn => \"Variance components\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n\n\nFigure 5.17: Kernel density estimate from bootstrap samples of the standard deviations for model m1L\n\n\n\n\nThe VC are all very nicely defined.\n\n\n5.7.3.3 Correlation parameters (CPs)\n\n\nCode\ndraw(\n  data(@subset(dat2, :type == \"ρ\")) *\n  mapping(\n    :value => \"Correlation\";\n    color=:names => rn => \"Correlation parameters\",\n  ) *\n  density();\n  figure=(; resolution=(800, 350)),\n)\n\n\n\n\n\nFigure 5.18: Kernel density estimate from bootstrap samples of the standard deviations for model m1L\n\n\n\n\nThree CPs stand out positively, the correlation between GM and the spatial effect, GM and attraction effect, and the correlation between spatial and attraction effects. The second CP was positive, but not significant in the first study. The third CP replicates a CP that was judged questionable in script kwdyz11.jl.\nThe three remaining CPs are not well defined for log-transformed reaction times; they only fit noise and should be removed. It is also possible that fitting the complex experimental design (including target size and rectangle orientation) will lead to more acceptable estimates. The corresponding plot based on LMM m1_rt for raw reaction times still shows them with very wide distributions, but acceptable.\n\n\nBaayen, H., Vasishth, S., Kliegl, R., & Bates, D. (2017). The cave of shadows: Addressing the human factor with generalized additive mixed models. Journal of Memory and Language, 94, 206–234. https://doi.org/10.1016/j.jml.2016.11.006\n\n\nBates, D., Kliegl, R., Vasishth, S., & Baayen, H. (2015). Parsimonius mixed models. arXiv, 1506.04967v1.\n\n\nKliegl, R., Kushela, J., & Laubrock, J. (2015). Object orientation and target size modulate the speed of visual attention. Department of Psychology, University of Potsdam.\n\n\nKliegl, R., Wei, P., Dambacher, M., Yan, M., & Zhou, X. (2010). Experimental effects and individual differences in linear mixed models: Estimating the relationship between spatial, object, and attraction effects in visual attention. Frontiers in Psychology. https://doi.org/10.3389/fpsyg.2010.00238"
  }
]